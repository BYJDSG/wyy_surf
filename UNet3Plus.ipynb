{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from diceLoss import dice_loss\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #查看label详情\n",
    "\n",
    "# # 读取灰度图像\n",
    "# image = cv2.imread('pancreas\\\\mask\\\\1c7493217d62.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # 将图像转换为 NumPy 数组\n",
    "# image_array = np.array(image)\n",
    "\n",
    "# # 打印图像矩阵的形状\n",
    "# print(f'Image shape: {image_array.shape}')\n",
    "\n",
    "# # 打印图像矩阵\n",
    "# print(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images，resize images，and split as train and test set\n",
    "\n",
    "def load_image_and_resize(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                if \"mask\" in folder:\n",
    "                    img = Image.open(filepath).convert('L').resize((320, 320))\n",
    "                    images.append(img)\n",
    "                else:\n",
    "                    img = Image.open(filepath).resize((320,320))\n",
    "                    images.append(img)\n",
    "            except IOError:\n",
    "                print(f\"Unable to open image file: {filename}\")\n",
    "    return images\n",
    "\n",
    "def load_image_and_resize_origin_mask(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                img = Image.open(filepath).resize((320,320))\n",
    "                images.append(img)\n",
    "            except IOError:\n",
    "                print(f\"Unable to open image file: {filename}\")\n",
    "    return images\n",
    "Image_for_pancreas = load_image_and_resize(\"pancreas\\\\image\")\n",
    "Mask_for_pancreas = load_image_and_resize(\"pancreas\\\\mask\")\n",
    "Image_for_stomach = load_image_and_resize(\"stomach\\\\image\")\n",
    "Mask_for_stomach = load_image_and_resize(\"stomach\\\\mask\")\n",
    "image_train_pancreas, image_test_pancreas, mask_train_pancreas, mask_test_pancreas = train_test_split(Image_for_pancreas, Mask_for_pancreas, test_size=0.3333, random_state=42)\n",
    "image_train_stomach, image_test_stomach, mask_train_stomach, mask_test_stomach = train_test_split(Image_for_stomach, Mask_for_stomach, test_size=0.3333, random_state=42)\n",
    "image_train = image_train_pancreas + image_train_stomach\n",
    "image_test = image_test_pancreas + image_test_stomach\n",
    "mask_train = mask_train_pancreas + mask_train_stomach\n",
    "mask_test = mask_test_pancreas + mask_test_stomach\n",
    "\n",
    "\n",
    "verify_Image_colorectum = load_image_and_resize(\"colorectum\\\\image\")\n",
    "verify_mask_colorectum = load_image_and_resize(\"colorectum\\\\mask\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dateset and Dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        # 如果调用时没有指定任何转换，我们将使用默认的转换\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.ToTensor(),  # 将PIL图像或NumPy ndarray转换为tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化处理\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()  # 通常情况下，遮罩只需要转换为tensor，不需要归一化\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image).to(device)\n",
    "        if self.mask_transform:\n",
    "            mask = (self.mask_transform(mask).to(device) > 0).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "# 创建训练集和测试集的 Dataset 对象\n",
    "train_dataset = CustomDataset(image_train, mask_train)\n",
    "test_dataset = CustomDataset(image_test, mask_test)\n",
    "verify_dataset = CustomDataset(verify_Image_colorectum, verify_mask_colorectum)\n",
    "\n",
    "# 可以使用 DataLoader 加载 Dataset 对象\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "verification_loader = DataLoader(verify_dataset, batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import unetConv2\n",
    "from init_weights import init_weights\n",
    "\n",
    "\n",
    "class UNet3Plus(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True, feature_scale=4,\n",
    "                 is_deconv=True, is_batchnorm=True):\n",
    "        super(UNet3Plus, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.feature_scale = feature_scale\n",
    "        self.is_deconv = is_deconv\n",
    "        self.is_batchnorm = is_batchnorm\n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        ## -------------Encoder--------------\n",
    "        self.conv1 = unetConv2(self.n_channels, filters[0], self.is_batchnorm)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv5 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
    "\n",
    "        ## -------------Decoder--------------\n",
    "        self.CatChannels = filters[0]\n",
    "        self.CatBlocks = 5\n",
    "        self.UpChannels = self.CatChannels * self.CatBlocks\n",
    "\n",
    "        '''stage 4d'''\n",
    "        # h1->320*320, hd4->40*40, Pooling 8 times\n",
    "        self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
    "        self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd4->40*40, Pooling 4 times\n",
    "        self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd4->40*40, Pooling 2 times\n",
    "        self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h4->40*40, hd4->40*40, Concatenation\n",
    "        self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
    "        self.h4_Cat_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->40*40, Upsample 2 times\n",
    "        self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4)\n",
    "        self.conv4d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn4d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu4d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 3d'''\n",
    "        # h1->320*320, hd3->80*80, Pooling 4 times\n",
    "        self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd3->80*80, Pooling 2 times\n",
    "        self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd3->80*80, Concatenation\n",
    "        self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_Cat_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd4->80*80, Upsample 2 times\n",
    "        self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->80*80, Upsample 4 times\n",
    "        self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)\n",
    "        self.conv3d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn3d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu3d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 2d '''\n",
    "        # h1->320*320, hd2->160*160, Pooling 2 times\n",
    "        self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd2->160*160, Concatenation\n",
    "        self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_Cat_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd2->160*160, Upsample 2 times\n",
    "        self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd2->160*160, Upsample 4 times\n",
    "        self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd2->160*160, Upsample 8 times\n",
    "        self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2)\n",
    "        self.conv2d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn2d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu2d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 1d'''\n",
    "        # h1->320*320, hd1->320*320, Concatenation\n",
    "        self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_Cat_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd2->160*160, hd1->320*320, Upsample 2 times\n",
    "        self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd2_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd1->320*320, Upsample 4 times\n",
    "        self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd1->320*320, Upsample 8 times\n",
    "        self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd1->320*320, Upsample 16 times\n",
    "        self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1)\n",
    "        self.conv1d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn1d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu1d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # output\n",
    "        self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
    "\n",
    "        # initialise weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## -------------Encoder-------------\n",
    "        h1 = self.conv1(inputs)  # h1->320*320*64\n",
    "\n",
    "        h2 = self.maxpool1(h1)\n",
    "        h2 = self.conv2(h2)  # h2->160*160*128\n",
    "\n",
    "        h3 = self.maxpool2(h2)\n",
    "        h3 = self.conv3(h3)  # h3->80*80*256\n",
    "\n",
    "        h4 = self.maxpool3(h3)\n",
    "        h4 = self.conv4(h4)  # h4->40*40*512\n",
    "\n",
    "        h5 = self.maxpool4(h4)\n",
    "        hd5 = self.conv5(h5)  # h5->20*20*1024\n",
    "\n",
    "        ## -------------Decoder-------------\n",
    "        h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_bn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
    "        h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_bn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
    "        h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_bn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
    "        h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_bn(self.h4_Cat_hd4_conv(h4)))\n",
    "        hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_bn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
    "        hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "\n",
    "        h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_bn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
    "        h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_bn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
    "        h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_bn(self.h3_Cat_hd3_conv(h3)))\n",
    "        hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_bn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
    "        hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_bn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
    "        hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)))) # hd3->80*80*UpChannels\n",
    "\n",
    "        h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_bn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
    "        h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_bn(self.h2_Cat_hd2_conv(h2)))\n",
    "        hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_bn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
    "        hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_bn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
    "        hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_bn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
    "        hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2), 1)))) # hd2->160*160*UpChannels\n",
    "\n",
    "        h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_bn(self.h1_Cat_hd1_conv(h1)))\n",
    "        hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_bn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
    "        hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_bn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
    "        hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_bn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
    "        hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_bn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
    "        hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels\n",
    "\n",
    "        d1 = self.outconv1(hd1)  # d1->320*320*n_classes\n",
    "        return F.sigmoid(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define criterion\n",
    "# # 创建 Dice Loss 函数\n",
    "# def dice_loss(pred, target, smooth=1e-2):\n",
    "#     \"\"\"\n",
    "#     Compute the DICE loss, which is 1 - Dice coefficient.\n",
    "#     Args:\n",
    "#         pred (tensor): the model's output, raw logits that have not been normalized.\n",
    "#         target (tensor): the ground truth labels.\n",
    "#         smooth (float): a smoothing constant to avoid division by zero.\n",
    "\n",
    "#     Returns:\n",
    "#         float: dice loss.\n",
    "#     \"\"\"\n",
    "#     intersection = (pred * target).sum(dim=(1, 2, 3))  # 计算每个样本的交集\n",
    "#     union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))  # 计算每个样本的并集\n",
    "\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)  # 计算Dice系数\n",
    "#     dice_loss = 1 - dice  # 计算Dice损失\n",
    "#     return dice_loss.mean()  # 返回批量的平均Dice损失\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "\n",
    "        output = model(data)  # 前向传播，获取模型输出\n",
    "\n",
    "        loss = criterion(output, target)  # 计算损失\n",
    "\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)  # 累积损失\n",
    "\n",
    "    # 计算平均损失\n",
    "    running_loss /= len(train_loader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Training Loss: {running_loss:.4f}')\n",
    "    \n",
    "def iou(input, target, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IOU) for two float tensors.\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): Predicted tensor.\n",
    "        target (torch.Tensor): Ground truth tensor.\n",
    "        eps (float): A small constant to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: IOU.\n",
    "    \"\"\"\n",
    "    input_flat = input.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "\n",
    "    intersection = torch.sum(input_flat * target_flat)\n",
    "    union = torch.sum(input_flat) + torch.sum(target_flat) - intersection\n",
    "\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    \n",
    "    return iou.item()\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total_iou = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss * data.size(0)\n",
    "            # 计算准确率\n",
    "            pred = (output > 0.5).float().squeeze()\n",
    "            target = target.squeeze()\n",
    "            correct += (pred.type(torch.int8) == target.type(torch.int8)).sum().item()\n",
    "\n",
    "\n",
    "            total_iou += iou(output, target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / (len(test_loader.dataset)*320*320)\n",
    "    avg_iou = total_iou / len(test_loader)\n",
    "\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)*320*320} ({accuracy:.2f}%), Avg IoU: {avg_iou:.4f}')\n",
    "    return test_loss, accuracy, avg_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7124\n",
      "Epoch: 2, Training Loss: 0.6132\n",
      "Epoch: 3, Training Loss: 0.6643\n",
      "Epoch: 4, Training Loss: 0.6220\n",
      "Epoch: 5, Training Loss: 0.5953\n",
      "Epoch: 6, Training Loss: 0.5886\n",
      "Epoch: 7, Training Loss: 0.5686\n",
      "Epoch: 8, Training Loss: 0.5625\n",
      "Epoch: 9, Training Loss: 0.5563\n",
      "Epoch: 10, Training Loss: 0.5462\n",
      "Model saved to ./models\\model_epoch_10.pth at epoch 10\n",
      "Epoch: 11, Training Loss: 0.5541\n",
      "Epoch: 12, Training Loss: 0.5533\n",
      "Epoch: 13, Training Loss: 0.5644\n",
      "Epoch: 14, Training Loss: 0.5547\n",
      "Epoch: 15, Training Loss: 0.5496\n",
      "Epoch: 16, Training Loss: 0.5440\n",
      "Epoch: 17, Training Loss: 0.5287\n",
      "Epoch: 18, Training Loss: 0.5303\n",
      "Epoch: 19, Training Loss: 0.5197\n",
      "Epoch: 20, Training Loss: 0.5096\n",
      "Model saved to ./models\\model_epoch_20.pth at epoch 20\n",
      "Epoch: 21, Training Loss: 0.4932\n",
      "Epoch: 22, Training Loss: 0.5094\n",
      "Epoch: 23, Training Loss: 0.5175\n",
      "Epoch: 24, Training Loss: 0.4804\n",
      "Epoch: 25, Training Loss: 0.4733\n",
      "Epoch: 26, Training Loss: 0.4485\n",
      "Epoch: 27, Training Loss: 0.4324\n",
      "Epoch: 28, Training Loss: 0.4816\n",
      "Epoch: 29, Training Loss: 0.4448\n",
      "Epoch: 30, Training Loss: 0.3989\n",
      "Model saved to ./models\\model_epoch_30.pth at epoch 30\n",
      "Epoch: 31, Training Loss: 0.5202\n",
      "Epoch: 32, Training Loss: 0.3982\n",
      "Epoch: 33, Training Loss: 0.3526\n",
      "Epoch: 34, Training Loss: 0.3917\n",
      "Epoch: 35, Training Loss: 0.3255\n",
      "Epoch: 36, Training Loss: 0.2787\n",
      "Epoch: 37, Training Loss: 0.2842\n",
      "Epoch: 38, Training Loss: 0.2569\n",
      "Epoch: 39, Training Loss: 0.2329\n",
      "Epoch: 40, Training Loss: 0.1949\n",
      "Model saved to ./models\\model_epoch_40.pth at epoch 40\n",
      "Epoch: 41, Training Loss: 0.1926\n",
      "Epoch: 42, Training Loss: 0.2027\n",
      "Epoch: 43, Training Loss: 0.1718\n",
      "Epoch: 44, Training Loss: 0.1539\n",
      "Epoch: 45, Training Loss: 0.1588\n",
      "Epoch: 46, Training Loss: 0.1555\n",
      "Epoch: 47, Training Loss: 0.1598\n",
      "Epoch: 48, Training Loss: 0.2091\n",
      "Epoch: 49, Training Loss: 0.1593\n",
      "Epoch: 50, Training Loss: 0.1266\n",
      "Model saved to ./models\\model_epoch_50.pth at epoch 50\n",
      "Epoch: 51, Training Loss: 0.1205\n",
      "Epoch: 52, Training Loss: 0.1477\n",
      "Epoch: 53, Training Loss: 0.1638\n",
      "Epoch: 54, Training Loss: 0.1243\n",
      "Epoch: 55, Training Loss: 0.0961\n",
      "Epoch: 56, Training Loss: 0.0837\n",
      "Epoch: 57, Training Loss: 0.0842\n",
      "Epoch: 58, Training Loss: 0.0890\n",
      "Epoch: 59, Training Loss: 0.0738\n",
      "Epoch: 60, Training Loss: 0.0824\n",
      "Model saved to ./models\\model_epoch_60.pth at epoch 60\n",
      "Epoch: 61, Training Loss: 0.0906\n",
      "Epoch: 62, Training Loss: 0.2396\n",
      "Epoch: 63, Training Loss: 0.5481\n",
      "Epoch: 64, Training Loss: 0.4220\n",
      "Epoch: 65, Training Loss: 0.3708\n",
      "Epoch: 66, Training Loss: 0.2618\n",
      "Epoch: 67, Training Loss: 0.2575\n",
      "Epoch: 68, Training Loss: 0.2070\n",
      "Epoch: 69, Training Loss: 0.1634\n",
      "Epoch: 70, Training Loss: 0.2485\n",
      "Model saved to ./models\\model_epoch_70.pth at epoch 70\n",
      "Epoch: 71, Training Loss: 0.1932\n",
      "Epoch: 72, Training Loss: 0.1295\n",
      "Epoch: 73, Training Loss: 0.1138\n",
      "Epoch: 74, Training Loss: 0.1091\n",
      "Epoch: 75, Training Loss: 0.0933\n",
      "Epoch: 76, Training Loss: 0.0840\n",
      "Epoch: 77, Training Loss: 0.0801\n",
      "Epoch: 78, Training Loss: 0.0852\n",
      "Epoch: 79, Training Loss: 0.0829\n",
      "Epoch: 80, Training Loss: 0.0775\n",
      "Model saved to ./models\\model_epoch_80.pth at epoch 80\n",
      "Epoch: 81, Training Loss: 0.0670\n",
      "Epoch: 82, Training Loss: 0.0587\n",
      "Epoch: 83, Training Loss: 0.0527\n",
      "Epoch: 84, Training Loss: 0.0491\n",
      "Epoch: 85, Training Loss: 0.0481\n",
      "Epoch: 86, Training Loss: 0.0546\n",
      "Epoch: 87, Training Loss: 0.4152\n",
      "Epoch: 88, Training Loss: 0.4206\n",
      "Epoch: 89, Training Loss: 0.3167\n",
      "Epoch: 90, Training Loss: 0.2204\n",
      "Model saved to ./models\\model_epoch_90.pth at epoch 90\n",
      "Epoch: 91, Training Loss: 0.1975\n",
      "Epoch: 92, Training Loss: 0.1453\n",
      "Epoch: 93, Training Loss: 0.1014\n",
      "Epoch: 94, Training Loss: 0.0867\n",
      "Epoch: 95, Training Loss: 0.0719\n",
      "Epoch: 96, Training Loss: 0.0650\n",
      "Epoch: 97, Training Loss: 0.0610\n",
      "Epoch: 98, Training Loss: 0.0560\n",
      "Epoch: 99, Training Loss: 0.0544\n",
      "Epoch: 100, Training Loss: 0.0563\n",
      "Model saved to ./models\\model_epoch_100.pth at epoch 100\n",
      "Epoch: 101, Training Loss: 0.0481\n",
      "Epoch: 102, Training Loss: 0.0415\n",
      "Epoch: 103, Training Loss: 0.0406\n",
      "Epoch: 104, Training Loss: 0.0388\n",
      "Epoch: 105, Training Loss: 0.0380\n",
      "Epoch: 106, Training Loss: 0.0545\n",
      "Epoch: 107, Training Loss: 0.1827\n",
      "Epoch: 108, Training Loss: 0.2832\n",
      "Epoch: 109, Training Loss: 0.1043\n",
      "Epoch: 110, Training Loss: 0.0801\n",
      "Model saved to ./models\\model_epoch_110.pth at epoch 110\n",
      "Epoch: 111, Training Loss: 0.0596\n",
      "Epoch: 112, Training Loss: 0.0502\n",
      "Epoch: 113, Training Loss: 0.0392\n",
      "Epoch: 114, Training Loss: 0.0361\n",
      "Epoch: 115, Training Loss: 0.0343\n",
      "Epoch: 116, Training Loss: 0.0433\n",
      "Epoch: 117, Training Loss: 0.0384\n",
      "Epoch: 118, Training Loss: 0.0439\n",
      "Epoch: 119, Training Loss: 0.0474\n",
      "Epoch: 120, Training Loss: 0.0432\n",
      "Model saved to ./models\\model_epoch_120.pth at epoch 120\n",
      "Epoch: 121, Training Loss: 0.0305\n",
      "Epoch: 122, Training Loss: 0.0273\n",
      "Epoch: 123, Training Loss: 0.0263\n",
      "Epoch: 124, Training Loss: 0.0256\n",
      "Epoch: 125, Training Loss: 0.0254\n",
      "Epoch: 126, Training Loss: 0.0292\n",
      "Epoch: 127, Training Loss: 0.0270\n",
      "Epoch: 128, Training Loss: 0.0269\n",
      "Epoch: 129, Training Loss: 0.0270\n",
      "Epoch: 130, Training Loss: 0.0367\n",
      "Model saved to ./models\\model_epoch_130.pth at epoch 130\n",
      "Epoch: 131, Training Loss: 0.0446\n",
      "Epoch: 132, Training Loss: 0.5294\n",
      "Epoch: 133, Training Loss: 0.3066\n",
      "Epoch: 134, Training Loss: 0.1718\n",
      "Epoch: 135, Training Loss: 0.1126\n",
      "Epoch: 136, Training Loss: 0.0820\n",
      "Epoch: 137, Training Loss: 0.0676\n",
      "Epoch: 138, Training Loss: 0.0489\n",
      "Epoch: 139, Training Loss: 0.0384\n",
      "Epoch: 140, Training Loss: 0.0336\n",
      "Model saved to ./models\\model_epoch_140.pth at epoch 140\n",
      "Epoch: 141, Training Loss: 0.0342\n",
      "Epoch: 142, Training Loss: 0.0329\n",
      "Epoch: 143, Training Loss: 0.0285\n",
      "Epoch: 144, Training Loss: 0.0289\n",
      "Epoch: 145, Training Loss: 0.0262\n",
      "Epoch: 146, Training Loss: 0.0301\n",
      "Epoch: 147, Training Loss: 0.0277\n",
      "Epoch: 148, Training Loss: 0.0317\n",
      "Epoch: 149, Training Loss: 0.0297\n",
      "Epoch: 150, Training Loss: 0.0276\n",
      "Model saved to ./models\\model_epoch_150.pth at epoch 150\n",
      "Epoch: 151, Training Loss: 0.0318\n",
      "Epoch: 152, Training Loss: 0.1155\n",
      "Epoch: 153, Training Loss: 0.2111\n",
      "Epoch: 154, Training Loss: 0.2168\n",
      "Epoch: 155, Training Loss: 0.1816\n",
      "Epoch: 156, Training Loss: 0.0917\n",
      "Epoch: 157, Training Loss: 0.0500\n",
      "Epoch: 158, Training Loss: 0.0405\n",
      "Epoch: 159, Training Loss: 0.0331\n",
      "Epoch: 160, Training Loss: 0.0283\n",
      "Model saved to ./models\\model_epoch_160.pth at epoch 160\n",
      "Epoch: 161, Training Loss: 0.0250\n",
      "Epoch: 162, Training Loss: 0.0228\n",
      "Epoch: 163, Training Loss: 0.0213\n",
      "Epoch: 164, Training Loss: 0.0201\n",
      "Epoch: 165, Training Loss: 0.0193\n",
      "Epoch: 166, Training Loss: 0.0193\n",
      "Epoch: 167, Training Loss: 0.0208\n",
      "Epoch: 168, Training Loss: 0.0257\n",
      "Epoch: 169, Training Loss: 0.0258\n",
      "Epoch: 170, Training Loss: 0.0245\n",
      "Model saved to ./models\\model_epoch_170.pth at epoch 170\n",
      "Epoch: 171, Training Loss: 0.3731\n",
      "Epoch: 172, Training Loss: 0.5646\n",
      "Epoch: 173, Training Loss: 0.4737\n",
      "Epoch: 174, Training Loss: 0.3138\n",
      "Epoch: 175, Training Loss: 0.2494\n",
      "Epoch: 176, Training Loss: 0.1831\n",
      "Epoch: 177, Training Loss: 0.1337\n",
      "Epoch: 178, Training Loss: 0.0836\n",
      "Epoch: 179, Training Loss: 0.0653\n",
      "Epoch: 180, Training Loss: 0.0499\n",
      "Model saved to ./models\\model_epoch_180.pth at epoch 180\n",
      "Epoch: 181, Training Loss: 0.0397\n",
      "Epoch: 182, Training Loss: 0.0341\n",
      "Epoch: 183, Training Loss: 0.0304\n",
      "Epoch: 184, Training Loss: 0.0271\n",
      "Epoch: 185, Training Loss: 0.0246\n",
      "Epoch: 186, Training Loss: 0.0235\n",
      "Epoch: 187, Training Loss: 0.0217\n",
      "Epoch: 188, Training Loss: 0.0206\n",
      "Epoch: 189, Training Loss: 0.0198\n",
      "Epoch: 190, Training Loss: 0.0192\n",
      "Model saved to ./models\\model_epoch_190.pth at epoch 190\n",
      "Epoch: 191, Training Loss: 0.0208\n",
      "Epoch: 192, Training Loss: 0.0222\n",
      "Epoch: 193, Training Loss: 0.0264\n",
      "Epoch: 194, Training Loss: 0.0260\n",
      "Epoch: 195, Training Loss: 0.0254\n",
      "Epoch: 196, Training Loss: 0.0222\n",
      "Epoch: 197, Training Loss: 0.0242\n",
      "Epoch: 198, Training Loss: 0.0241\n",
      "Epoch: 199, Training Loss: 0.0224\n",
      "Epoch: 200, Training Loss: 0.0207\n",
      "Model saved to ./models\\model_epoch_200.pth at epoch 200\n",
      "Epoch: 201, Training Loss: 0.0271\n",
      "Epoch: 202, Training Loss: 0.0286\n",
      "Epoch: 203, Training Loss: 0.0219\n",
      "Epoch: 204, Training Loss: 0.0182\n",
      "Epoch: 205, Training Loss: 0.0167\n",
      "Epoch: 206, Training Loss: 0.0170\n",
      "Epoch: 207, Training Loss: 0.0177\n",
      "Epoch: 208, Training Loss: 0.0205\n",
      "Epoch: 209, Training Loss: 0.0239\n",
      "Epoch: 210, Training Loss: 0.0254\n",
      "Model saved to ./models\\model_epoch_210.pth at epoch 210\n",
      "Epoch: 211, Training Loss: 0.0219\n",
      "Epoch: 212, Training Loss: 0.0174\n",
      "Epoch: 213, Training Loss: 0.0167\n",
      "Epoch: 214, Training Loss: 0.0181\n",
      "Epoch: 215, Training Loss: 0.0161\n",
      "Epoch: 216, Training Loss: 0.0155\n",
      "Epoch: 217, Training Loss: 0.0185\n",
      "Epoch: 218, Training Loss: 0.0653\n",
      "Epoch: 219, Training Loss: 0.5459\n",
      "Epoch: 220, Training Loss: 0.3009\n",
      "Model saved to ./models\\model_epoch_220.pth at epoch 220\n",
      "Epoch: 221, Training Loss: 0.1361\n",
      "Epoch: 222, Training Loss: 0.0758\n",
      "Epoch: 223, Training Loss: 0.0470\n",
      "Epoch: 224, Training Loss: 0.0337\n",
      "Epoch: 225, Training Loss: 0.0270\n",
      "Epoch: 226, Training Loss: 0.0227\n",
      "Epoch: 227, Training Loss: 0.0197\n",
      "Epoch: 228, Training Loss: 0.0178\n",
      "Epoch: 229, Training Loss: 0.0163\n",
      "Epoch: 230, Training Loss: 0.0155\n",
      "Model saved to ./models\\model_epoch_230.pth at epoch 230\n",
      "Epoch: 231, Training Loss: 0.0147\n",
      "Epoch: 232, Training Loss: 0.0146\n",
      "Epoch: 233, Training Loss: 0.0144\n",
      "Epoch: 234, Training Loss: 0.0162\n",
      "Epoch: 235, Training Loss: 0.0175\n",
      "Epoch: 236, Training Loss: 0.0165\n",
      "Epoch: 237, Training Loss: 0.0172\n",
      "Epoch: 238, Training Loss: 0.0158\n",
      "Epoch: 239, Training Loss: 0.0158\n",
      "Epoch: 240, Training Loss: 0.0161\n",
      "Model saved to ./models\\model_epoch_240.pth at epoch 240\n",
      "Epoch: 241, Training Loss: 0.0159\n",
      "Epoch: 242, Training Loss: 0.0165\n",
      "Epoch: 243, Training Loss: 0.0162\n",
      "Epoch: 244, Training Loss: 0.0198\n",
      "Epoch: 245, Training Loss: 0.0195\n",
      "Epoch: 246, Training Loss: 0.0173\n",
      "Epoch: 247, Training Loss: 0.0177\n",
      "Epoch: 248, Training Loss: 0.0158\n",
      "Epoch: 249, Training Loss: 0.0156\n",
      "Epoch: 250, Training Loss: 0.0148\n",
      "Model saved to ./models\\model_epoch_250.pth at epoch 250\n",
      "Epoch: 251, Training Loss: 0.0130\n",
      "Epoch: 252, Training Loss: 0.0143\n",
      "Epoch: 253, Training Loss: 0.0149\n",
      "Epoch: 254, Training Loss: 0.0147\n",
      "Epoch: 255, Training Loss: 0.0168\n",
      "Epoch: 256, Training Loss: 0.0166\n",
      "Epoch: 257, Training Loss: 0.0199\n",
      "Epoch: 258, Training Loss: 0.0203\n",
      "Epoch: 259, Training Loss: 0.0279\n",
      "Epoch: 260, Training Loss: 0.0287\n",
      "Model saved to ./models\\model_epoch_260.pth at epoch 260\n",
      "Epoch: 261, Training Loss: 0.6809\n",
      "Epoch: 262, Training Loss: 0.5011\n",
      "Epoch: 263, Training Loss: 0.4038\n",
      "Epoch: 264, Training Loss: 0.2551\n",
      "Epoch: 265, Training Loss: 0.1718\n",
      "Epoch: 266, Training Loss: 0.1236\n",
      "Epoch: 267, Training Loss: 0.0848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 执行训练\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[32], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# 设置模型为训练模式\u001b[39;00m\n\u001b[0;32m     25\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清除之前的梯度\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)  \u001b[38;5;66;03m# 前向传播，获取模型输出\u001b[39;00m\n",
      "File \u001b[1;32md:\\application\\anaconda\\envs\\pytorch_10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\application\\anaconda\\envs\\pytorch_10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\application\\anaconda\\envs\\pytorch_10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\application\\anaconda\\envs\\pytorch_10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[index]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_transform:\n\u001b[0;32m     25\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_transform(mask)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型初始化\n",
    "\n",
    "model = UNet3Plus().to(device)\n",
    "\n",
    "# 优化器配置\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 损失函数配置\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 训练周期数\n",
    "epochs = 300\n",
    "\n",
    "# 执行训练\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        if os.path.exists('./models') is False:\n",
    "            os.makedirs('./models')\n",
    "        model_path = os.path.join(\"./models\", f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model_path)\n",
    "        print(f'Model saved to {model_path} at epoch {epoch+1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6221, Accuracy: 2926309/4096000 (71.44%), Avg IoU: 0.2693\n",
      "Model model_epoch_10.pth tested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijunbao\\AppData\\Local\\Temp\\ipykernel_48844\\1244852301.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_info_save = pd.concat([test_info_save, new_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6989, Accuracy: 2641408/4096000 (64.49%), Avg IoU: 0.1930\n",
      "Model model_epoch_20.pth tested.\n",
      "Test set: Average loss: 0.6833, Accuracy: 2886969/4096000 (70.48%), Avg IoU: 0.2109\n",
      "Model model_epoch_30.pth tested.\n",
      "Test set: Average loss: 0.6101, Accuracy: 3138490/4096000 (76.62%), Avg IoU: 0.2823\n",
      "Model model_epoch_40.pth tested.\n",
      "Test set: Average loss: 0.8311, Accuracy: 2804801/4096000 (68.48%), Avg IoU: 0.1006\n",
      "Model model_epoch_50.pth tested.\n",
      "Test set: Average loss: 0.8518, Accuracy: 2779570/4096000 (67.86%), Avg IoU: 0.0906\n",
      "Model model_epoch_60.pth tested.\n",
      "Test set: Average loss: 0.6293, Accuracy: 2955646/4096000 (72.16%), Avg IoU: 0.2572\n",
      "Model model_epoch_70.pth tested.\n",
      "Test set: Average loss: 0.5828, Accuracy: 3143491/4096000 (76.75%), Avg IoU: 0.3166\n",
      "Model model_epoch_80.pth tested.\n",
      "Test set: Average loss: 0.7144, Accuracy: 2902185/4096000 (70.85%), Avg IoU: 0.1883\n",
      "Model model_epoch_90.pth tested.\n",
      "Test set: Average loss: 0.6365, Accuracy: 3125836/4096000 (76.31%), Avg IoU: 0.2668\n",
      "Model model_epoch_100.pth tested.\n",
      "Test set: Average loss: 0.6069, Accuracy: 3152946/4096000 (76.98%), Avg IoU: 0.2890\n",
      "Model model_epoch_110.pth tested.\n",
      "Test set: Average loss: 0.5880, Accuracy: 3160249/4096000 (77.15%), Avg IoU: 0.3090\n",
      "Model model_epoch_120.pth tested.\n",
      "Test set: Average loss: 0.7003, Accuracy: 2982123/4096000 (72.81%), Avg IoU: 0.2163\n",
      "Model model_epoch_130.pth tested.\n",
      "Test set: Average loss: 0.6279, Accuracy: 3057891/4096000 (74.66%), Avg IoU: 0.2716\n",
      "Model model_epoch_140.pth tested.\n",
      "Test set: Average loss: 0.5531, Accuracy: 3136440/4096000 (76.57%), Avg IoU: 0.3457\n",
      "Model model_epoch_150.pth tested.\n",
      "Test set: Average loss: 0.5291, Accuracy: 3071656/4096000 (74.99%), Avg IoU: 0.3727\n",
      "Model model_epoch_160.pth tested.\n",
      "Test set: Average loss: 0.7045, Accuracy: 3056300/4096000 (74.62%), Avg IoU: 0.2184\n",
      "Model model_epoch_170.pth tested.\n",
      "Test set: Average loss: 0.5877, Accuracy: 3019755/4096000 (73.72%), Avg IoU: 0.3151\n",
      "Model model_epoch_180.pth tested.\n",
      "Test set: Average loss: 0.5979, Accuracy: 3011301/4096000 (73.52%), Avg IoU: 0.3047\n",
      "Model model_epoch_190.pth tested.\n",
      "Test set: Average loss: 0.5927, Accuracy: 3070765/4096000 (74.97%), Avg IoU: 0.3120\n",
      "Model model_epoch_200.pth tested.\n",
      "Test set: Average loss: 0.5937, Accuracy: 2917143/4096000 (71.22%), Avg IoU: 0.3031\n",
      "Model model_epoch_210.pth tested.\n",
      "Test set: Average loss: 0.6834, Accuracy: 2743732/4096000 (66.99%), Avg IoU: 0.2080\n",
      "Model model_epoch_220.pth tested.\n",
      "Test set: Average loss: 0.5802, Accuracy: 3149930/4096000 (76.90%), Avg IoU: 0.3240\n",
      "Model model_epoch_230.pth tested.\n",
      "Test set: Average loss: 0.6402, Accuracy: 3039791/4096000 (74.21%), Avg IoU: 0.2696\n",
      "Model model_epoch_240.pth tested.\n",
      "Test set: Average loss: 0.5605, Accuracy: 3085078/4096000 (75.32%), Avg IoU: 0.3359\n",
      "Model model_epoch_250.pth tested.\n",
      "Test set: Average loss: 0.5612, Accuracy: 3206151/4096000 (78.28%), Avg IoU: 0.3543\n",
      "Model model_epoch_260.pth tested.\n",
      "Test set: Average loss: 0.6055, Accuracy: 3110003/4096000 (75.93%), Avg IoU: 0.2979\n",
      "Model model_epoch_270.pth tested.\n",
      "Test set: Average loss: 0.5702, Accuracy: 3110534/4096000 (75.94%), Avg IoU: 0.3315\n",
      "Model model_epoch_280.pth tested.\n",
      "Test set: Average loss: 0.6315, Accuracy: 3102279/4096000 (75.74%), Avg IoU: 0.2725\n",
      "Model model_epoch_290.pth tested.\n",
      "Test set: Average loss: 0.5303, Accuracy: 3122682/4096000 (76.24%), Avg IoU: 0.3669\n",
      "Model model_epoch_300.pth tested.\n",
      "Test set: Average loss: 0.5828, Accuracy: 2922909/4096000 (71.36%), Avg IoU: 0.3122\n",
      "Model model_epoch_310.pth tested.\n",
      "Test set: Average loss: 0.5491, Accuracy: 3111447/4096000 (75.96%), Avg IoU: 0.3447\n",
      "Model model_epoch_320.pth tested.\n",
      "Test set: Average loss: 0.6129, Accuracy: 3078002/4096000 (75.15%), Avg IoU: 0.2850\n",
      "Model model_epoch_330.pth tested.\n",
      "Test set: Average loss: 0.5853, Accuracy: 3138334/4096000 (76.62%), Avg IoU: 0.3152\n",
      "Model model_epoch_340.pth tested.\n",
      "Test set: Average loss: 0.5995, Accuracy: 3157766/4096000 (77.09%), Avg IoU: 0.3028\n",
      "Model model_epoch_350.pth tested.\n",
      "Test set: Average loss: 0.5673, Accuracy: 3149478/4096000 (76.89%), Avg IoU: 0.3340\n",
      "Model model_epoch_360.pth tested.\n",
      "Test set: Average loss: 0.6023, Accuracy: 3174327/4096000 (77.50%), Avg IoU: 0.3041\n",
      "Model model_epoch_370.pth tested.\n",
      "Test set: Average loss: 0.5535, Accuracy: 3230243/4096000 (78.86%), Avg IoU: 0.3460\n",
      "Model model_epoch_380.pth tested.\n",
      "Test set: Average loss: 0.5396, Accuracy: 3210090/4096000 (78.37%), Avg IoU: 0.3606\n",
      "Model model_epoch_390.pth tested.\n",
      "Test set: Average loss: 0.5666, Accuracy: 3195408/4096000 (78.01%), Avg IoU: 0.3394\n",
      "Model model_epoch_400.pth tested.\n",
      "Test set: Average loss: 0.5602, Accuracy: 3194656/4096000 (77.99%), Avg IoU: 0.3419\n",
      "Model model_epoch_410.pth tested.\n",
      "Test set: Average loss: 0.5568, Accuracy: 3166158/4096000 (77.30%), Avg IoU: 0.3467\n",
      "Model model_epoch_420.pth tested.\n",
      "Test set: Average loss: 0.6094, Accuracy: 3117143/4096000 (76.10%), Avg IoU: 0.2946\n",
      "Model model_epoch_430.pth tested.\n",
      "Test set: Average loss: 0.6063, Accuracy: 3049390/4096000 (74.45%), Avg IoU: 0.2948\n",
      "Model model_epoch_440.pth tested.\n",
      "Test set: Average loss: 0.7286, Accuracy: 3036445/4096000 (74.13%), Avg IoU: 0.1934\n",
      "Model model_epoch_450.pth tested.\n",
      "Test set: Average loss: 0.5393, Accuracy: 2957848/4096000 (72.21%), Avg IoU: 0.3613\n",
      "Model model_epoch_460.pth tested.\n",
      "Test set: Average loss: 0.5862, Accuracy: 3055511/4096000 (74.60%), Avg IoU: 0.3209\n",
      "Model model_epoch_470.pth tested.\n",
      "Test set: Average loss: 0.6212, Accuracy: 3079723/4096000 (75.19%), Avg IoU: 0.2800\n",
      "Model model_epoch_480.pth tested.\n",
      "Test set: Average loss: 0.5827, Accuracy: 3133306/4096000 (76.50%), Avg IoU: 0.3233\n",
      "Model model_epoch_490.pth tested.\n",
      "Test set: Average loss: 0.5975, Accuracy: 3126713/4096000 (76.34%), Avg IoU: 0.3056\n",
      "Model model_epoch_500.pth tested.\n",
      "Test set: Average loss: 0.5846, Accuracy: 3185026/4096000 (77.76%), Avg IoU: 0.3194\n",
      "Model model_epoch_510.pth tested.\n",
      "Test set: Average loss: 0.6236, Accuracy: 3085442/4096000 (75.33%), Avg IoU: 0.2805\n",
      "Model model_epoch_520.pth tested.\n",
      "Test set: Average loss: 0.5721, Accuracy: 3111671/4096000 (75.97%), Avg IoU: 0.3283\n",
      "Model model_epoch_530.pth tested.\n",
      "Test set: Average loss: 0.6157, Accuracy: 3117538/4096000 (76.11%), Avg IoU: 0.2927\n",
      "Model model_epoch_540.pth tested.\n",
      "Test set: Average loss: 0.5861, Accuracy: 3130389/4096000 (76.43%), Avg IoU: 0.3148\n",
      "Model model_epoch_550.pth tested.\n",
      "Test set: Average loss: 0.5624, Accuracy: 3136475/4096000 (76.57%), Avg IoU: 0.3425\n",
      "Model model_epoch_560.pth tested.\n",
      "Test set: Average loss: 0.6234, Accuracy: 3117946/4096000 (76.12%), Avg IoU: 0.2877\n",
      "Model model_epoch_570.pth tested.\n",
      "Test set: Average loss: 0.5713, Accuracy: 3146512/4096000 (76.82%), Avg IoU: 0.3332\n",
      "Model model_epoch_580.pth tested.\n",
      "Test set: Average loss: 0.5604, Accuracy: 3116037/4096000 (76.08%), Avg IoU: 0.3367\n",
      "Model model_epoch_590.pth tested.\n",
      "Test set: Average loss: 0.5451, Accuracy: 3128193/4096000 (76.37%), Avg IoU: 0.3548\n",
      "Model model_epoch_600.pth tested.\n",
      "Test set: Average loss: 0.5665, Accuracy: 3046260/4096000 (74.37%), Avg IoU: 0.3334\n",
      "Model model_epoch_610.pth tested.\n",
      "Test set: Average loss: 0.6185, Accuracy: 2980924/4096000 (72.78%), Avg IoU: 0.2853\n",
      "Model model_epoch_620.pth tested.\n",
      "Test set: Average loss: 0.5658, Accuracy: 3031639/4096000 (74.01%), Avg IoU: 0.3330\n",
      "Model model_epoch_630.pth tested.\n",
      "Test set: Average loss: 0.5481, Accuracy: 3024329/4096000 (73.84%), Avg IoU: 0.3491\n",
      "Model model_epoch_640.pth tested.\n",
      "Test set: Average loss: 0.5607, Accuracy: 3018693/4096000 (73.70%), Avg IoU: 0.3340\n",
      "Model model_epoch_650.pth tested.\n",
      "Test set: Average loss: 0.5850, Accuracy: 3112600/4096000 (75.99%), Avg IoU: 0.3171\n",
      "Model model_epoch_660.pth tested.\n",
      "Test set: Average loss: 0.5956, Accuracy: 3045817/4096000 (74.36%), Avg IoU: 0.3051\n",
      "Model model_epoch_670.pth tested.\n",
      "Test set: Average loss: 0.6402, Accuracy: 2640803/4096000 (64.47%), Avg IoU: 0.2512\n",
      "Model model_epoch_680.pth tested.\n",
      "Test set: Average loss: 0.5775, Accuracy: 3075324/4096000 (75.08%), Avg IoU: 0.3245\n",
      "Model model_epoch_690.pth tested.\n",
      "Test set: Average loss: 0.5798, Accuracy: 3049878/4096000 (74.46%), Avg IoU: 0.3288\n",
      "Model model_epoch_700.pth tested.\n",
      "Test set: Average loss: 0.5625, Accuracy: 3083702/4096000 (75.29%), Avg IoU: 0.3374\n",
      "Model model_epoch_710.pth tested.\n",
      "Test set: Average loss: 0.5204, Accuracy: 3075354/4096000 (75.08%), Avg IoU: 0.3807\n",
      "Model model_epoch_720.pth tested.\n",
      "Test set: Average loss: 0.5534, Accuracy: 3070092/4096000 (74.95%), Avg IoU: 0.3484\n",
      "Model model_epoch_730.pth tested.\n",
      "Test set: Average loss: 0.5861, Accuracy: 3092130/4096000 (75.49%), Avg IoU: 0.3146\n",
      "Model model_epoch_740.pth tested.\n",
      "Test set: Average loss: 0.8165, Accuracy: 2389255/4096000 (58.33%), Avg IoU: 0.1228\n",
      "Model model_epoch_750.pth tested.\n",
      "Test set: Average loss: 0.6083, Accuracy: 3066707/4096000 (74.87%), Avg IoU: 0.2893\n",
      "Model model_epoch_760.pth tested.\n",
      "Test set: Average loss: 0.5963, Accuracy: 3097333/4096000 (75.62%), Avg IoU: 0.3080\n",
      "Model model_epoch_770.pth tested.\n",
      "Test set: Average loss: 0.5743, Accuracy: 3114245/4096000 (76.03%), Avg IoU: 0.3237\n",
      "Model model_epoch_780.pth tested.\n",
      "Test set: Average loss: 0.5934, Accuracy: 3090850/4096000 (75.46%), Avg IoU: 0.3048\n",
      "Model model_epoch_790.pth tested.\n",
      "Test set: Average loss: 0.5863, Accuracy: 3117959/4096000 (76.12%), Avg IoU: 0.3100\n",
      "Model model_epoch_800.pth tested.\n",
      "Test set: Average loss: 0.4967, Accuracy: 3033199/4096000 (74.05%), Avg IoU: 0.4030\n",
      "Model model_epoch_810.pth tested.\n",
      "Test set: Average loss: 0.5336, Accuracy: 3172535/4096000 (77.45%), Avg IoU: 0.3664\n",
      "Model model_epoch_820.pth tested.\n",
      "Test set: Average loss: 0.5524, Accuracy: 3105176/4096000 (75.81%), Avg IoU: 0.3469\n",
      "Model model_epoch_830.pth tested.\n",
      "Test set: Average loss: 0.5930, Accuracy: 3118084/4096000 (76.13%), Avg IoU: 0.3009\n",
      "Model model_epoch_840.pth tested.\n",
      "Test set: Average loss: 0.6390, Accuracy: 3057250/4096000 (74.64%), Avg IoU: 0.2727\n",
      "Model model_epoch_850.pth tested.\n",
      "Test set: Average loss: 0.5351, Accuracy: 3071443/4096000 (74.99%), Avg IoU: 0.3655\n",
      "Model model_epoch_860.pth tested.\n",
      "Test set: Average loss: 0.5340, Accuracy: 3112636/4096000 (75.99%), Avg IoU: 0.3673\n",
      "Model model_epoch_870.pth tested.\n",
      "Test set: Average loss: 0.5470, Accuracy: 3140396/4096000 (76.67%), Avg IoU: 0.3536\n",
      "Model model_epoch_880.pth tested.\n",
      "Test set: Average loss: 0.5819, Accuracy: 3112587/4096000 (75.99%), Avg IoU: 0.3221\n",
      "Model model_epoch_890.pth tested.\n",
      "Test set: Average loss: 0.5579, Accuracy: 3144697/4096000 (76.77%), Avg IoU: 0.3473\n",
      "Model model_epoch_900.pth tested.\n"
     ]
    }
   ],
   "source": [
    "#test and save the relevant information\n",
    "import os\n",
    "import pandas as pd\n",
    "model_epochs = []\n",
    "for model_name in os.listdir('models'):\n",
    "    if model_name.endswith('.pth'):\n",
    "        model_epochs.append(int(model_name.split('_')[2][0:-4]))\n",
    "model_epochs.sort()\n",
    "\n",
    "test_info_save = pd.DataFrame(columns=['epoch', 'dice_loss', 'acc', 'iou'])\n",
    "\n",
    "for model_epoch in model_epochs:\n",
    "    model_name = 'model_epoch_' + str(model_epoch) + '.pth'\n",
    "    if model_name.endswith('.pth'):\n",
    "        checkpoint_path = os.path.join('models', model_name)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        testloss, accuracy, avg_iou = test(model, test_loader, dice_loss)\n",
    "        new_data = pd.DataFrame({'epoch': [model_epoch], \n",
    "                         'dice_loss': [testloss], \n",
    "                         'acc': [accuracy], \n",
    "                         'iou': [avg_iou]})\n",
    "        # 使用 concat 来添加新行\n",
    "        test_info_save = pd.concat([test_info_save, new_data], ignore_index=True)\n",
    "        test_info_save.to_csv('test_info_save_second_round_900epochs.csv', index=False)\n",
    "        print('Model {} tested.'.format(model_name))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4388, Accuracy: 4712528/6144000 (76.70%), Avg IoU: 0.4128\n",
      "Model model_epoch_10.pth tested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yijunbao\\AppData\\Local\\Temp\\ipykernel_48844\\81598185.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  verify_info_save = pd.concat([verify_info_save, new_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.5556, Accuracy: 3535314/6144000 (57.54%), Avg IoU: 0.2976\n",
      "Model model_epoch_20.pth tested.\n",
      "Test set: Average loss: 0.4437, Accuracy: 4534668/6144000 (73.81%), Avg IoU: 0.4112\n",
      "Model model_epoch_30.pth tested.\n",
      "Test set: Average loss: 0.3973, Accuracy: 4780231/6144000 (77.80%), Avg IoU: 0.4674\n",
      "Model model_epoch_40.pth tested.\n",
      "Test set: Average loss: 0.6408, Accuracy: 3861920/6144000 (62.86%), Avg IoU: 0.2405\n",
      "Model model_epoch_50.pth tested.\n",
      "Test set: Average loss: 0.6079, Accuracy: 4014391/6144000 (65.34%), Avg IoU: 0.2813\n",
      "Model model_epoch_60.pth tested.\n",
      "Test set: Average loss: 0.4729, Accuracy: 4365786/6144000 (71.06%), Avg IoU: 0.3850\n",
      "Model model_epoch_70.pth tested.\n",
      "Test set: Average loss: 0.3994, Accuracy: 4676335/6144000 (76.11%), Avg IoU: 0.4814\n",
      "Model model_epoch_80.pth tested.\n",
      "Test set: Average loss: 0.4912, Accuracy: 4328332/6144000 (70.45%), Avg IoU: 0.3703\n",
      "Model model_epoch_90.pth tested.\n",
      "Test set: Average loss: 0.4224, Accuracy: 4681001/6144000 (76.19%), Avg IoU: 0.4555\n",
      "Model model_epoch_100.pth tested.\n",
      "Test set: Average loss: 0.4637, Accuracy: 4552454/6144000 (74.10%), Avg IoU: 0.4115\n",
      "Model model_epoch_110.pth tested.\n",
      "Test set: Average loss: 0.4416, Accuracy: 4554275/6144000 (74.13%), Avg IoU: 0.4357\n",
      "Model model_epoch_120.pth tested.\n",
      "Test set: Average loss: 0.4560, Accuracy: 4491955/6144000 (73.11%), Avg IoU: 0.4214\n",
      "Model model_epoch_130.pth tested.\n",
      "Test set: Average loss: 0.5163, Accuracy: 4269747/6144000 (69.49%), Avg IoU: 0.3641\n",
      "Model model_epoch_140.pth tested.\n",
      "Test set: Average loss: 0.4046, Accuracy: 4620032/6144000 (75.20%), Avg IoU: 0.4763\n",
      "Model model_epoch_150.pth tested.\n",
      "Test set: Average loss: 0.3812, Accuracy: 4655397/6144000 (75.77%), Avg IoU: 0.5068\n",
      "Model model_epoch_160.pth tested.\n",
      "Test set: Average loss: 0.4870, Accuracy: 4479370/6144000 (72.91%), Avg IoU: 0.3977\n",
      "Model model_epoch_170.pth tested.\n",
      "Test set: Average loss: 0.4361, Accuracy: 4534962/6144000 (73.81%), Avg IoU: 0.4478\n",
      "Model model_epoch_180.pth tested.\n",
      "Test set: Average loss: 0.4540, Accuracy: 4430636/6144000 (72.11%), Avg IoU: 0.4277\n",
      "Model model_epoch_190.pth tested.\n",
      "Test set: Average loss: 0.4581, Accuracy: 4444727/6144000 (72.34%), Avg IoU: 0.4259\n",
      "Model model_epoch_200.pth tested.\n",
      "Test set: Average loss: 0.4201, Accuracy: 4462799/6144000 (72.64%), Avg IoU: 0.4658\n",
      "Model model_epoch_210.pth tested.\n",
      "Test set: Average loss: 0.5215, Accuracy: 4163451/6144000 (67.76%), Avg IoU: 0.3351\n",
      "Model model_epoch_220.pth tested.\n",
      "Test set: Average loss: 0.3465, Accuracy: 4933330/6144000 (80.30%), Avg IoU: 0.5412\n",
      "Model model_epoch_230.pth tested.\n",
      "Test set: Average loss: 0.4454, Accuracy: 4561893/6144000 (74.25%), Avg IoU: 0.4369\n",
      "Model model_epoch_240.pth tested.\n",
      "Test set: Average loss: 0.3742, Accuracy: 4725926/6144000 (76.92%), Avg IoU: 0.5097\n",
      "Model model_epoch_250.pth tested.\n",
      "Test set: Average loss: 0.3958, Accuracy: 4736937/6144000 (77.10%), Avg IoU: 0.4945\n",
      "Model model_epoch_260.pth tested.\n",
      "Test set: Average loss: 0.4628, Accuracy: 4454692/6144000 (72.50%), Avg IoU: 0.4245\n",
      "Model model_epoch_270.pth tested.\n",
      "Test set: Average loss: 0.3941, Accuracy: 4669756/6144000 (76.01%), Avg IoU: 0.4942\n",
      "Model model_epoch_280.pth tested.\n",
      "Test set: Average loss: 0.5096, Accuracy: 4350445/6144000 (70.81%), Avg IoU: 0.3782\n",
      "Model model_epoch_290.pth tested.\n",
      "Test set: Average loss: 0.3406, Accuracy: 4783335/6144000 (77.85%), Avg IoU: 0.5463\n",
      "Model model_epoch_300.pth tested.\n",
      "Test set: Average loss: 0.4236, Accuracy: 4666382/6144000 (75.95%), Avg IoU: 0.4335\n",
      "Model model_epoch_310.pth tested.\n",
      "Test set: Average loss: 0.3715, Accuracy: 4692010/6144000 (76.37%), Avg IoU: 0.5068\n",
      "Model model_epoch_320.pth tested.\n",
      "Test set: Average loss: 0.4884, Accuracy: 4374922/6144000 (71.21%), Avg IoU: 0.3945\n",
      "Model model_epoch_330.pth tested.\n",
      "Test set: Average loss: 0.4107, Accuracy: 4669806/6144000 (76.01%), Avg IoU: 0.4772\n",
      "Model model_epoch_340.pth tested.\n",
      "Test set: Average loss: 0.4479, Accuracy: 4553683/6144000 (74.12%), Avg IoU: 0.4404\n",
      "Model model_epoch_350.pth tested.\n",
      "Test set: Average loss: 0.3482, Accuracy: 4864863/6144000 (79.18%), Avg IoU: 0.5452\n",
      "Model model_epoch_360.pth tested.\n",
      "Test set: Average loss: 0.4835, Accuracy: 4435763/6144000 (72.20%), Avg IoU: 0.4052\n",
      "Model model_epoch_370.pth tested.\n",
      "Test set: Average loss: 0.4228, Accuracy: 4667580/6144000 (75.97%), Avg IoU: 0.4579\n",
      "Model model_epoch_380.pth tested.\n",
      "Test set: Average loss: 0.3580, Accuracy: 4852243/6144000 (78.98%), Avg IoU: 0.5362\n",
      "Model model_epoch_390.pth tested.\n",
      "Test set: Average loss: 0.4315, Accuracy: 4646628/6144000 (75.63%), Avg IoU: 0.4644\n",
      "Model model_epoch_400.pth tested.\n",
      "Test set: Average loss: 0.3985, Accuracy: 4723308/6144000 (76.88%), Avg IoU: 0.4910\n",
      "Model model_epoch_410.pth tested.\n",
      "Test set: Average loss: 0.4140, Accuracy: 4624262/6144000 (75.26%), Avg IoU: 0.4725\n",
      "Model model_epoch_420.pth tested.\n",
      "Test set: Average loss: 0.4653, Accuracy: 4448117/6144000 (72.40%), Avg IoU: 0.4092\n",
      "Model model_epoch_430.pth tested.\n",
      "Test set: Average loss: 0.4554, Accuracy: 4438232/6144000 (72.24%), Avg IoU: 0.4249\n",
      "Model model_epoch_440.pth tested.\n",
      "Test set: Average loss: 0.6189, Accuracy: 4058422/6144000 (66.06%), Avg IoU: 0.2718\n",
      "Model model_epoch_450.pth tested.\n",
      "Test set: Average loss: 0.3420, Accuracy: 4649447/6144000 (75.67%), Avg IoU: 0.5404\n",
      "Model model_epoch_460.pth tested.\n",
      "Test set: Average loss: 0.3918, Accuracy: 4661369/6144000 (75.87%), Avg IoU: 0.4926\n",
      "Model model_epoch_470.pth tested.\n",
      "Test set: Average loss: 0.4549, Accuracy: 4443666/6144000 (72.33%), Avg IoU: 0.4244\n",
      "Model model_epoch_480.pth tested.\n",
      "Test set: Average loss: 0.3949, Accuracy: 4692065/6144000 (76.37%), Avg IoU: 0.4948\n",
      "Model model_epoch_490.pth tested.\n",
      "Test set: Average loss: 0.4103, Accuracy: 4649492/6144000 (75.68%), Avg IoU: 0.4751\n",
      "Model model_epoch_500.pth tested.\n",
      "Test set: Average loss: 0.3804, Accuracy: 4778305/6144000 (77.77%), Avg IoU: 0.5090\n",
      "Model model_epoch_510.pth tested.\n",
      "Test set: Average loss: 0.4716, Accuracy: 4500497/6144000 (73.25%), Avg IoU: 0.4140\n",
      "Model model_epoch_520.pth tested.\n",
      "Test set: Average loss: 0.3946, Accuracy: 4674842/6144000 (76.09%), Avg IoU: 0.4920\n",
      "Model model_epoch_530.pth tested.\n",
      "Test set: Average loss: 0.4473, Accuracy: 4584891/6144000 (74.62%), Avg IoU: 0.4423\n",
      "Model model_epoch_540.pth tested.\n",
      "Test set: Average loss: 0.4182, Accuracy: 4629452/6144000 (75.35%), Avg IoU: 0.4688\n",
      "Model model_epoch_550.pth tested.\n",
      "Test set: Average loss: 0.3347, Accuracy: 4948556/6144000 (80.54%), Avg IoU: 0.5650\n",
      "Model model_epoch_560.pth tested.\n",
      "Test set: Average loss: 0.4194, Accuracy: 4659866/6144000 (75.84%), Avg IoU: 0.4711\n",
      "Model model_epoch_570.pth tested.\n",
      "Test set: Average loss: 0.3929, Accuracy: 4716717/6144000 (76.77%), Avg IoU: 0.4974\n",
      "Model model_epoch_580.pth tested.\n",
      "Test set: Average loss: 0.3653, Accuracy: 4776983/6144000 (77.75%), Avg IoU: 0.5225\n",
      "Model model_epoch_590.pth tested.\n",
      "Test set: Average loss: 0.3694, Accuracy: 4728692/6144000 (76.96%), Avg IoU: 0.5202\n",
      "Model model_epoch_600.pth tested.\n",
      "Test set: Average loss: 0.4225, Accuracy: 4588655/6144000 (74.69%), Avg IoU: 0.4679\n",
      "Model model_epoch_610.pth tested.\n",
      "Test set: Average loss: 0.5255, Accuracy: 4212041/6144000 (68.56%), Avg IoU: 0.3679\n",
      "Model model_epoch_620.pth tested.\n",
      "Test set: Average loss: 0.4390, Accuracy: 4445626/6144000 (72.36%), Avg IoU: 0.4473\n",
      "Model model_epoch_630.pth tested.\n",
      "Test set: Average loss: 0.4087, Accuracy: 4530888/6144000 (73.74%), Avg IoU: 0.4778\n",
      "Model model_epoch_640.pth tested.\n",
      "Test set: Average loss: 0.3734, Accuracy: 4736049/6144000 (77.08%), Avg IoU: 0.5198\n",
      "Model model_epoch_650.pth tested.\n",
      "Test set: Average loss: 0.3969, Accuracy: 4715716/6144000 (76.75%), Avg IoU: 0.4979\n",
      "Model model_epoch_660.pth tested.\n",
      "Test set: Average loss: 0.4993, Accuracy: 4305300/6144000 (70.07%), Avg IoU: 0.3870\n",
      "Model model_epoch_670.pth tested.\n",
      "Test set: Average loss: 0.4355, Accuracy: 4040835/6144000 (65.77%), Avg IoU: 0.4293\n",
      "Model model_epoch_680.pth tested.\n",
      "Test set: Average loss: 0.3888, Accuracy: 4733318/6144000 (77.04%), Avg IoU: 0.5024\n",
      "Model model_epoch_690.pth tested.\n",
      "Test set: Average loss: 0.3967, Accuracy: 4706143/6144000 (76.60%), Avg IoU: 0.5000\n",
      "Model model_epoch_700.pth tested.\n",
      "Test set: Average loss: 0.3737, Accuracy: 4747688/6144000 (77.27%), Avg IoU: 0.5155\n",
      "Model model_epoch_710.pth tested.\n",
      "Test set: Average loss: 0.3102, Accuracy: 4924414/6144000 (80.15%), Avg IoU: 0.5899\n",
      "Model model_epoch_720.pth tested.\n",
      "Test set: Average loss: 0.3886, Accuracy: 4714505/6144000 (76.73%), Avg IoU: 0.5045\n",
      "Model model_epoch_730.pth tested.\n",
      "Test set: Average loss: 0.3975, Accuracy: 4695943/6144000 (76.43%), Avg IoU: 0.4941\n",
      "Model model_epoch_740.pth tested.\n",
      "Test set: Average loss: 0.7082, Accuracy: 3345871/6144000 (54.46%), Avg IoU: 0.1894\n",
      "Model model_epoch_750.pth tested.\n",
      "Test set: Average loss: 0.4734, Accuracy: 4439610/6144000 (72.26%), Avg IoU: 0.4107\n",
      "Model model_epoch_760.pth tested.\n",
      "Test set: Average loss: 0.4674, Accuracy: 4504670/6144000 (73.32%), Avg IoU: 0.4241\n",
      "Model model_epoch_770.pth tested.\n",
      "Test set: Average loss: 0.4176, Accuracy: 4608844/6144000 (75.01%), Avg IoU: 0.4691\n",
      "Model model_epoch_780.pth tested.\n",
      "Test set: Average loss: 0.4539, Accuracy: 4525444/6144000 (73.66%), Avg IoU: 0.4338\n",
      "Model model_epoch_790.pth tested.\n",
      "Test set: Average loss: 0.4258, Accuracy: 4559647/6144000 (74.21%), Avg IoU: 0.4566\n",
      "Model model_epoch_800.pth tested.\n",
      "Test set: Average loss: 0.3326, Accuracy: 4696174/6144000 (76.44%), Avg IoU: 0.5547\n",
      "Model model_epoch_810.pth tested.\n",
      "Test set: Average loss: 0.3568, Accuracy: 4812229/6144000 (78.32%), Avg IoU: 0.5345\n",
      "Model model_epoch_820.pth tested.\n",
      "Test set: Average loss: 0.3747, Accuracy: 4720889/6144000 (76.84%), Avg IoU: 0.5159\n",
      "Model model_epoch_830.pth tested.\n",
      "Test set: Average loss: 0.4258, Accuracy: 4555614/6144000 (74.15%), Avg IoU: 0.4425\n",
      "Model model_epoch_840.pth tested.\n",
      "Test set: Average loss: 0.4767, Accuracy: 4487723/6144000 (73.04%), Avg IoU: 0.4148\n",
      "Model model_epoch_850.pth tested.\n",
      "Test set: Average loss: 0.3700, Accuracy: 4667093/6144000 (75.96%), Avg IoU: 0.5122\n",
      "Model model_epoch_860.pth tested.\n",
      "Test set: Average loss: 0.3486, Accuracy: 4793893/6144000 (78.03%), Avg IoU: 0.5422\n",
      "Model model_epoch_870.pth tested.\n",
      "Test set: Average loss: 0.3487, Accuracy: 4848009/6144000 (78.91%), Avg IoU: 0.5442\n",
      "Model model_epoch_880.pth tested.\n",
      "Test set: Average loss: 0.4161, Accuracy: 4643595/6144000 (75.58%), Avg IoU: 0.4725\n",
      "Model model_epoch_890.pth tested.\n",
      "Test set: Average loss: 0.4107, Accuracy: 4634147/6144000 (75.43%), Avg IoU: 0.4785\n",
      "Model model_epoch_900.pth tested.\n"
     ]
    }
   ],
   "source": [
    "#verify and save the relevant information\n",
    "import os\n",
    "import pandas as pd\n",
    "model_epochs = []\n",
    "for model_name in os.listdir('models'):\n",
    "    if model_name.endswith('.pth'):\n",
    "        model_epochs.append(int(model_name.split('_')[2][0:-4]))\n",
    "model_epochs.sort()\n",
    "\n",
    "verify_info_save = pd.DataFrame(columns=['epoch', 'dice_loss', 'acc', 'iou'])\n",
    "\n",
    "for model_epoch in model_epochs:\n",
    "    model_name = 'model_epoch_' + str(model_epoch) + '.pth'\n",
    "    if model_name.endswith('.pth'):\n",
    "        checkpoint_path = os.path.join('models', model_name)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        verifyloss, accuracy, avg_iou = test(model, verification_loader, dice_loss)\n",
    "        new_data = pd.DataFrame({'epoch': [model_epoch], \n",
    "                         'dice_loss': [verifyloss], \n",
    "                         'acc': [accuracy], \n",
    "                         'iou': [avg_iou]})\n",
    "        # 使用 concat 来添加新行\n",
    "        verify_info_save = pd.concat([verify_info_save, new_data], ignore_index=True)\n",
    "        verify_info_save.to_csv('verify_info_save_second_round_900epochs.csv', index=False)\n",
    "        print('Model {} tested.'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301, Training Loss: 0.0149\n",
      "Epoch: 302, Training Loss: 0.0143\n",
      "Epoch: 303, Training Loss: 0.0138\n",
      "Epoch: 304, Training Loss: 0.0125\n",
      "Epoch: 305, Training Loss: 0.0113\n",
      "Epoch: 306, Training Loss: 0.0117\n",
      "Epoch: 307, Training Loss: 0.0142\n",
      "Epoch: 308, Training Loss: 0.0208\n",
      "Epoch: 309, Training Loss: 0.0382\n",
      "Epoch: 310, Training Loss: 0.9960\n",
      "Model saved to ./models\\model_epoch_310.pth at epoch 310\n",
      "Epoch: 311, Training Loss: 0.5535\n",
      "Epoch: 312, Training Loss: 0.3221\n",
      "Epoch: 313, Training Loss: 0.1908\n",
      "Epoch: 314, Training Loss: 0.1092\n",
      "Epoch: 315, Training Loss: 0.0628\n",
      "Epoch: 316, Training Loss: 0.0377\n",
      "Epoch: 317, Training Loss: 0.0271\n",
      "Epoch: 318, Training Loss: 0.0217\n",
      "Epoch: 319, Training Loss: 0.0181\n",
      "Epoch: 320, Training Loss: 0.0158\n",
      "Model saved to ./models\\model_epoch_320.pth at epoch 320\n",
      "Epoch: 321, Training Loss: 0.0142\n",
      "Epoch: 322, Training Loss: 0.0130\n",
      "Epoch: 323, Training Loss: 0.0120\n",
      "Epoch: 324, Training Loss: 0.0112\n",
      "Epoch: 325, Training Loss: 0.0106\n",
      "Epoch: 326, Training Loss: 0.0105\n",
      "Epoch: 327, Training Loss: 0.0100\n",
      "Epoch: 328, Training Loss: 0.0097\n",
      "Epoch: 329, Training Loss: 0.0107\n",
      "Epoch: 330, Training Loss: 0.0114\n",
      "Model saved to ./models\\model_epoch_330.pth at epoch 330\n",
      "Epoch: 331, Training Loss: 0.0133\n",
      "Epoch: 332, Training Loss: 0.0134\n",
      "Epoch: 333, Training Loss: 0.0138\n",
      "Epoch: 334, Training Loss: 0.0123\n",
      "Epoch: 335, Training Loss: 0.0143\n",
      "Epoch: 336, Training Loss: 0.0130\n",
      "Epoch: 337, Training Loss: 0.0114\n",
      "Epoch: 338, Training Loss: 0.0107\n",
      "Epoch: 339, Training Loss: 0.0110\n",
      "Epoch: 340, Training Loss: 0.0117\n",
      "Model saved to ./models\\model_epoch_340.pth at epoch 340\n",
      "Epoch: 341, Training Loss: 0.0104\n",
      "Epoch: 342, Training Loss: 0.0100\n",
      "Epoch: 343, Training Loss: 0.0103\n",
      "Epoch: 344, Training Loss: 0.0110\n",
      "Epoch: 345, Training Loss: 0.0118\n",
      "Epoch: 346, Training Loss: 0.0152\n",
      "Epoch: 347, Training Loss: 0.0144\n",
      "Epoch: 348, Training Loss: 0.0129\n",
      "Epoch: 349, Training Loss: 0.0113\n",
      "Epoch: 350, Training Loss: 0.0110\n",
      "Model saved to ./models\\model_epoch_350.pth at epoch 350\n",
      "Epoch: 351, Training Loss: 0.0108\n",
      "Epoch: 352, Training Loss: 0.0095\n",
      "Epoch: 353, Training Loss: 0.0096\n",
      "Epoch: 354, Training Loss: 0.0122\n",
      "Epoch: 355, Training Loss: 0.0127\n",
      "Epoch: 356, Training Loss: 0.0144\n",
      "Epoch: 357, Training Loss: 0.0149\n",
      "Epoch: 358, Training Loss: 0.0136\n",
      "Epoch: 359, Training Loss: 0.0130\n",
      "Epoch: 360, Training Loss: 0.0125\n",
      "Model saved to ./models\\model_epoch_360.pth at epoch 360\n",
      "Epoch: 361, Training Loss: 0.0119\n",
      "Epoch: 362, Training Loss: 0.0118\n",
      "Epoch: 363, Training Loss: 0.0104\n",
      "Epoch: 364, Training Loss: 0.0124\n",
      "Epoch: 365, Training Loss: 0.0124\n",
      "Epoch: 366, Training Loss: 0.0112\n",
      "Epoch: 367, Training Loss: 0.0096\n",
      "Epoch: 368, Training Loss: 0.0100\n",
      "Epoch: 369, Training Loss: 0.0100\n",
      "Epoch: 370, Training Loss: 0.0131\n",
      "Model saved to ./models\\model_epoch_370.pth at epoch 370\n",
      "Epoch: 371, Training Loss: 0.0133\n",
      "Epoch: 372, Training Loss: 0.0125\n",
      "Epoch: 373, Training Loss: 0.0124\n",
      "Epoch: 374, Training Loss: 0.0119\n",
      "Epoch: 375, Training Loss: 0.0127\n",
      "Epoch: 376, Training Loss: 1.1878\n",
      "Epoch: 377, Training Loss: 0.3380\n",
      "Epoch: 378, Training Loss: 0.1275\n",
      "Epoch: 379, Training Loss: 0.0648\n",
      "Epoch: 380, Training Loss: 0.0430\n",
      "Model saved to ./models\\model_epoch_380.pth at epoch 380\n",
      "Epoch: 381, Training Loss: 0.0322\n",
      "Epoch: 382, Training Loss: 0.0246\n",
      "Epoch: 383, Training Loss: 0.0201\n",
      "Epoch: 384, Training Loss: 0.0171\n",
      "Epoch: 385, Training Loss: 0.0150\n",
      "Epoch: 386, Training Loss: 0.0132\n",
      "Epoch: 387, Training Loss: 0.0120\n",
      "Epoch: 388, Training Loss: 0.0110\n",
      "Epoch: 389, Training Loss: 0.0102\n",
      "Epoch: 390, Training Loss: 0.0095\n",
      "Model saved to ./models\\model_epoch_390.pth at epoch 390\n",
      "Epoch: 391, Training Loss: 0.0088\n",
      "Epoch: 392, Training Loss: 0.0086\n",
      "Epoch: 393, Training Loss: 0.0085\n",
      "Epoch: 394, Training Loss: 0.0086\n",
      "Epoch: 395, Training Loss: 0.0083\n",
      "Epoch: 396, Training Loss: 0.0088\n",
      "Epoch: 397, Training Loss: 0.0091\n",
      "Epoch: 398, Training Loss: 0.0087\n",
      "Epoch: 399, Training Loss: 0.0086\n",
      "Epoch: 400, Training Loss: 0.0096\n",
      "Model saved to ./models\\model_epoch_400.pth at epoch 400\n",
      "Epoch: 401, Training Loss: 0.0101\n",
      "Epoch: 402, Training Loss: 0.0103\n",
      "Epoch: 403, Training Loss: 0.0108\n",
      "Epoch: 404, Training Loss: 0.0136\n",
      "Epoch: 405, Training Loss: 0.0138\n",
      "Epoch: 406, Training Loss: 0.0137\n",
      "Epoch: 407, Training Loss: 0.0106\n",
      "Epoch: 408, Training Loss: 0.0096\n",
      "Epoch: 409, Training Loss: 0.0090\n",
      "Epoch: 410, Training Loss: 0.0086\n",
      "Model saved to ./models\\model_epoch_410.pth at epoch 410\n",
      "Epoch: 411, Training Loss: 0.0077\n",
      "Epoch: 412, Training Loss: 0.0086\n",
      "Epoch: 413, Training Loss: 0.0102\n",
      "Epoch: 414, Training Loss: 0.0106\n",
      "Epoch: 415, Training Loss: 0.0123\n",
      "Epoch: 416, Training Loss: 0.0150\n",
      "Epoch: 417, Training Loss: 0.0194\n",
      "Epoch: 418, Training Loss: 0.0126\n",
      "Epoch: 419, Training Loss: 0.0101\n",
      "Epoch: 420, Training Loss: 0.0088\n",
      "Model saved to ./models\\model_epoch_420.pth at epoch 420\n",
      "Epoch: 421, Training Loss: 0.0089\n",
      "Epoch: 422, Training Loss: 0.0099\n",
      "Epoch: 423, Training Loss: 0.0084\n",
      "Epoch: 424, Training Loss: 0.0087\n",
      "Epoch: 425, Training Loss: 0.0103\n",
      "Epoch: 426, Training Loss: 0.0111\n",
      "Epoch: 427, Training Loss: 0.0112\n",
      "Epoch: 428, Training Loss: 0.0096\n",
      "Epoch: 429, Training Loss: 0.0114\n",
      "Epoch: 430, Training Loss: 0.0137\n",
      "Model saved to ./models\\model_epoch_430.pth at epoch 430\n",
      "Epoch: 431, Training Loss: 0.0157\n",
      "Epoch: 432, Training Loss: 0.0117\n",
      "Epoch: 433, Training Loss: 0.0093\n",
      "Epoch: 434, Training Loss: 0.0076\n",
      "Epoch: 435, Training Loss: 0.0067\n",
      "Epoch: 436, Training Loss: 0.0065\n",
      "Epoch: 437, Training Loss: 0.0071\n",
      "Epoch: 438, Training Loss: 0.0081\n",
      "Epoch: 439, Training Loss: 0.0094\n",
      "Epoch: 440, Training Loss: 0.0106\n",
      "Model saved to ./models\\model_epoch_440.pth at epoch 440\n",
      "Epoch: 441, Training Loss: 0.0136\n",
      "Epoch: 442, Training Loss: 0.0149\n",
      "Epoch: 443, Training Loss: 0.2772\n",
      "Epoch: 444, Training Loss: 0.2089\n",
      "Epoch: 445, Training Loss: 0.0667\n",
      "Epoch: 446, Training Loss: 0.0423\n",
      "Epoch: 447, Training Loss: 0.0256\n",
      "Epoch: 448, Training Loss: 0.0184\n",
      "Epoch: 449, Training Loss: 0.0150\n",
      "Epoch: 450, Training Loss: 0.0127\n",
      "Model saved to ./models\\model_epoch_450.pth at epoch 450\n",
      "Epoch: 451, Training Loss: 0.0113\n",
      "Epoch: 452, Training Loss: 0.0101\n",
      "Epoch: 453, Training Loss: 0.0092\n",
      "Epoch: 454, Training Loss: 0.0085\n",
      "Epoch: 455, Training Loss: 0.0078\n",
      "Epoch: 456, Training Loss: 0.0074\n",
      "Epoch: 457, Training Loss: 0.0069\n",
      "Epoch: 458, Training Loss: 0.0067\n",
      "Epoch: 459, Training Loss: 0.0067\n",
      "Epoch: 460, Training Loss: 0.0070\n",
      "Model saved to ./models\\model_epoch_460.pth at epoch 460\n",
      "Epoch: 461, Training Loss: 0.0074\n",
      "Epoch: 462, Training Loss: 0.0096\n",
      "Epoch: 463, Training Loss: 0.0090\n",
      "Epoch: 464, Training Loss: 0.0109\n",
      "Epoch: 465, Training Loss: 0.0091\n",
      "Epoch: 466, Training Loss: 0.0079\n",
      "Epoch: 467, Training Loss: 0.0071\n",
      "Epoch: 468, Training Loss: 0.0065\n",
      "Epoch: 469, Training Loss: 0.0067\n",
      "Epoch: 470, Training Loss: 0.0072\n",
      "Model saved to ./models\\model_epoch_470.pth at epoch 470\n",
      "Epoch: 471, Training Loss: 0.0075\n",
      "Epoch: 472, Training Loss: 0.0078\n",
      "Epoch: 473, Training Loss: 0.0080\n",
      "Epoch: 474, Training Loss: 0.0099\n",
      "Epoch: 475, Training Loss: 0.0100\n",
      "Epoch: 476, Training Loss: 0.0094\n",
      "Epoch: 477, Training Loss: 0.0082\n",
      "Epoch: 478, Training Loss: 0.0085\n",
      "Epoch: 479, Training Loss: 0.0081\n",
      "Epoch: 480, Training Loss: 0.0078\n",
      "Model saved to ./models\\model_epoch_480.pth at epoch 480\n",
      "Epoch: 481, Training Loss: 0.0086\n",
      "Epoch: 482, Training Loss: 0.0094\n",
      "Epoch: 483, Training Loss: 0.0101\n",
      "Epoch: 484, Training Loss: 0.0095\n",
      "Epoch: 485, Training Loss: 0.0100\n",
      "Epoch: 486, Training Loss: 0.0109\n",
      "Epoch: 487, Training Loss: 0.0109\n",
      "Epoch: 488, Training Loss: 0.0177\n",
      "Epoch: 489, Training Loss: 0.0142\n",
      "Epoch: 490, Training Loss: 0.0119\n",
      "Model saved to ./models\\model_epoch_490.pth at epoch 490\n",
      "Epoch: 491, Training Loss: 0.0088\n",
      "Epoch: 492, Training Loss: 0.0068\n",
      "Epoch: 493, Training Loss: 0.0061\n",
      "Epoch: 494, Training Loss: 0.0062\n",
      "Epoch: 495, Training Loss: 0.0070\n",
      "Epoch: 496, Training Loss: 0.0079\n",
      "Epoch: 497, Training Loss: 0.0097\n",
      "Epoch: 498, Training Loss: 0.0102\n",
      "Epoch: 499, Training Loss: 0.0102\n",
      "Epoch: 500, Training Loss: 0.0096\n",
      "Model saved to ./models\\model_epoch_500.pth at epoch 500\n",
      "Epoch: 501, Training Loss: 0.0087\n",
      "Epoch: 502, Training Loss: 0.0077\n",
      "Epoch: 503, Training Loss: 0.0070\n",
      "Epoch: 504, Training Loss: 0.0082\n",
      "Epoch: 505, Training Loss: 0.0095\n",
      "Epoch: 506, Training Loss: 0.0095\n",
      "Epoch: 507, Training Loss: 0.0083\n",
      "Epoch: 508, Training Loss: 0.0070\n",
      "Epoch: 509, Training Loss: 0.0069\n",
      "Epoch: 510, Training Loss: 0.0067\n",
      "Model saved to ./models\\model_epoch_510.pth at epoch 510\n",
      "Epoch: 511, Training Loss: 0.0078\n",
      "Epoch: 512, Training Loss: 0.0084\n",
      "Epoch: 513, Training Loss: 0.0087\n",
      "Epoch: 514, Training Loss: 0.0330\n",
      "Epoch: 515, Training Loss: 0.1624\n",
      "Epoch: 516, Training Loss: 0.2424\n",
      "Epoch: 517, Training Loss: 0.0798\n",
      "Epoch: 518, Training Loss: 0.0369\n",
      "Epoch: 519, Training Loss: 0.0237\n",
      "Epoch: 520, Training Loss: 0.0182\n",
      "Model saved to ./models\\model_epoch_520.pth at epoch 520\n",
      "Epoch: 521, Training Loss: 0.0147\n",
      "Epoch: 522, Training Loss: 0.0126\n",
      "Epoch: 523, Training Loss: 0.0111\n",
      "Epoch: 524, Training Loss: 0.0098\n",
      "Epoch: 525, Training Loss: 0.0089\n",
      "Epoch: 526, Training Loss: 0.0081\n",
      "Epoch: 527, Training Loss: 0.0075\n",
      "Epoch: 528, Training Loss: 0.0069\n",
      "Epoch: 529, Training Loss: 0.0064\n",
      "Epoch: 530, Training Loss: 0.0061\n",
      "Model saved to ./models\\model_epoch_530.pth at epoch 530\n",
      "Epoch: 531, Training Loss: 0.0058\n",
      "Epoch: 532, Training Loss: 0.0058\n",
      "Epoch: 533, Training Loss: 0.0060\n",
      "Epoch: 534, Training Loss: 0.0064\n",
      "Epoch: 535, Training Loss: 0.0067\n",
      "Epoch: 536, Training Loss: 0.0094\n",
      "Epoch: 537, Training Loss: 0.0099\n",
      "Epoch: 538, Training Loss: 0.0094\n",
      "Epoch: 539, Training Loss: 0.0081\n",
      "Epoch: 540, Training Loss: 0.0068\n",
      "Model saved to ./models\\model_epoch_540.pth at epoch 540\n",
      "Epoch: 541, Training Loss: 0.0062\n",
      "Epoch: 542, Training Loss: 0.0058\n",
      "Epoch: 543, Training Loss: 0.0057\n",
      "Epoch: 544, Training Loss: 0.0057\n",
      "Epoch: 545, Training Loss: 0.0059\n",
      "Epoch: 546, Training Loss: 0.0076\n",
      "Epoch: 547, Training Loss: 0.0081\n",
      "Epoch: 548, Training Loss: 0.0071\n",
      "Epoch: 549, Training Loss: 0.0066\n",
      "Epoch: 550, Training Loss: 0.0070\n",
      "Model saved to ./models\\model_epoch_550.pth at epoch 550\n",
      "Epoch: 551, Training Loss: 0.0071\n",
      "Epoch: 552, Training Loss: 0.0075\n",
      "Epoch: 553, Training Loss: 0.0072\n",
      "Epoch: 554, Training Loss: 0.0069\n",
      "Epoch: 555, Training Loss: 0.0060\n",
      "Epoch: 556, Training Loss: 0.0061\n",
      "Epoch: 557, Training Loss: 0.0062\n",
      "Epoch: 558, Training Loss: 0.0078\n",
      "Epoch: 559, Training Loss: 0.0081\n",
      "Epoch: 560, Training Loss: 0.0103\n",
      "Model saved to ./models\\model_epoch_560.pth at epoch 560\n",
      "Epoch: 561, Training Loss: 0.0102\n",
      "Epoch: 562, Training Loss: 0.0108\n",
      "Epoch: 563, Training Loss: 0.0084\n",
      "Epoch: 564, Training Loss: 0.0065\n",
      "Epoch: 565, Training Loss: 0.0067\n",
      "Epoch: 566, Training Loss: 0.0063\n",
      "Epoch: 567, Training Loss: 0.0063\n",
      "Epoch: 568, Training Loss: 0.0064\n",
      "Epoch: 569, Training Loss: 0.0062\n",
      "Epoch: 570, Training Loss: 0.0062\n",
      "Model saved to ./models\\model_epoch_570.pth at epoch 570\n",
      "Epoch: 571, Training Loss: 0.0067\n",
      "Epoch: 572, Training Loss: 0.0076\n",
      "Epoch: 573, Training Loss: 0.0089\n",
      "Epoch: 574, Training Loss: 0.0084\n",
      "Epoch: 575, Training Loss: 0.0070\n",
      "Epoch: 576, Training Loss: 0.0061\n",
      "Epoch: 577, Training Loss: 0.0056\n",
      "Epoch: 578, Training Loss: 0.0061\n",
      "Epoch: 579, Training Loss: 0.0062\n",
      "Epoch: 580, Training Loss: 0.0069\n",
      "Model saved to ./models\\model_epoch_580.pth at epoch 580\n",
      "Epoch: 581, Training Loss: 0.0081\n",
      "Epoch: 582, Training Loss: 0.0087\n",
      "Epoch: 583, Training Loss: 0.0072\n",
      "Epoch: 584, Training Loss: 0.0075\n",
      "Epoch: 585, Training Loss: 0.0073\n",
      "Epoch: 586, Training Loss: 0.0079\n",
      "Epoch: 587, Training Loss: 0.0079\n",
      "Epoch: 588, Training Loss: 0.0064\n",
      "Epoch: 589, Training Loss: 0.0058\n",
      "Epoch: 590, Training Loss: 0.0064\n",
      "Model saved to ./models\\model_epoch_590.pth at epoch 590\n",
      "Epoch: 591, Training Loss: 0.0081\n",
      "Epoch: 592, Training Loss: 0.0077\n",
      "Epoch: 593, Training Loss: 0.0079\n",
      "Epoch: 594, Training Loss: 0.0061\n",
      "Epoch: 595, Training Loss: 0.0058\n",
      "Epoch: 596, Training Loss: 0.0065\n",
      "Epoch: 597, Training Loss: 0.0080\n",
      "Epoch: 598, Training Loss: 0.0085\n",
      "Epoch: 599, Training Loss: 0.0075\n",
      "Epoch: 600, Training Loss: 0.0103\n",
      "Model saved to ./models\\model_epoch_600.pth at epoch 600\n",
      "Epoch: 601, Training Loss: 0.4195\n",
      "Epoch: 602, Training Loss: 0.0923\n",
      "Epoch: 603, Training Loss: 0.0352\n",
      "Epoch: 604, Training Loss: 0.0225\n",
      "Epoch: 605, Training Loss: 0.0169\n",
      "Epoch: 606, Training Loss: 0.0139\n",
      "Epoch: 607, Training Loss: 0.0118\n",
      "Epoch: 608, Training Loss: 0.0103\n",
      "Epoch: 609, Training Loss: 0.0091\n",
      "Epoch: 610, Training Loss: 0.0083\n",
      "Model saved to ./models\\model_epoch_610.pth at epoch 610\n",
      "Epoch: 611, Training Loss: 0.0075\n",
      "Epoch: 612, Training Loss: 0.0068\n",
      "Epoch: 613, Training Loss: 0.0063\n",
      "Epoch: 614, Training Loss: 0.0059\n",
      "Epoch: 615, Training Loss: 0.0055\n",
      "Epoch: 616, Training Loss: 0.0051\n",
      "Epoch: 617, Training Loss: 0.0048\n",
      "Epoch: 618, Training Loss: 0.0045\n",
      "Epoch: 619, Training Loss: 0.0043\n",
      "Epoch: 620, Training Loss: 0.0041\n",
      "Model saved to ./models\\model_epoch_620.pth at epoch 620\n",
      "Epoch: 621, Training Loss: 0.0039\n",
      "Epoch: 622, Training Loss: 0.0038\n",
      "Epoch: 623, Training Loss: 0.0039\n",
      "Epoch: 624, Training Loss: 0.0038\n",
      "Epoch: 625, Training Loss: 0.0037\n",
      "Epoch: 626, Training Loss: 0.0042\n",
      "Epoch: 627, Training Loss: 0.0047\n",
      "Epoch: 628, Training Loss: 0.0054\n",
      "Epoch: 629, Training Loss: 0.0056\n",
      "Epoch: 630, Training Loss: 0.0065\n",
      "Model saved to ./models\\model_epoch_630.pth at epoch 630\n",
      "Epoch: 631, Training Loss: 0.0077\n",
      "Epoch: 632, Training Loss: 0.0057\n",
      "Epoch: 633, Training Loss: 0.0046\n",
      "Epoch: 634, Training Loss: 0.0042\n",
      "Epoch: 635, Training Loss: 0.0039\n",
      "Epoch: 636, Training Loss: 0.0037\n",
      "Epoch: 637, Training Loss: 0.0039\n",
      "Epoch: 638, Training Loss: 0.0051\n",
      "Epoch: 639, Training Loss: 0.0050\n",
      "Epoch: 640, Training Loss: 0.0047\n",
      "Model saved to ./models\\model_epoch_640.pth at epoch 640\n",
      "Epoch: 641, Training Loss: 0.0052\n",
      "Epoch: 642, Training Loss: 0.0072\n",
      "Epoch: 643, Training Loss: 0.0083\n",
      "Epoch: 644, Training Loss: 0.0064\n",
      "Epoch: 645, Training Loss: 0.0054\n",
      "Epoch: 646, Training Loss: 0.0050\n",
      "Epoch: 647, Training Loss: 0.0042\n",
      "Epoch: 648, Training Loss: 0.0038\n",
      "Epoch: 649, Training Loss: 0.0041\n",
      "Epoch: 650, Training Loss: 0.0045\n",
      "Model saved to ./models\\model_epoch_650.pth at epoch 650\n",
      "Epoch: 651, Training Loss: 0.0053\n",
      "Epoch: 652, Training Loss: 0.0056\n",
      "Epoch: 653, Training Loss: 0.0057\n",
      "Epoch: 654, Training Loss: 0.0050\n",
      "Epoch: 655, Training Loss: 0.0049\n",
      "Epoch: 656, Training Loss: 0.0056\n",
      "Epoch: 657, Training Loss: 0.0057\n",
      "Epoch: 658, Training Loss: 0.0050\n",
      "Epoch: 659, Training Loss: 0.0048\n",
      "Epoch: 660, Training Loss: 0.0057\n",
      "Model saved to ./models\\model_epoch_660.pth at epoch 660\n",
      "Epoch: 661, Training Loss: 0.0055\n",
      "Epoch: 662, Training Loss: 0.0050\n",
      "Epoch: 663, Training Loss: 0.0047\n",
      "Epoch: 664, Training Loss: 0.0042\n",
      "Epoch: 665, Training Loss: 0.0043\n",
      "Epoch: 666, Training Loss: 0.0050\n",
      "Epoch: 667, Training Loss: 0.0052\n",
      "Epoch: 668, Training Loss: 0.0079\n",
      "Epoch: 669, Training Loss: 0.0077\n",
      "Epoch: 670, Training Loss: 0.0090\n",
      "Model saved to ./models\\model_epoch_670.pth at epoch 670\n",
      "Epoch: 671, Training Loss: 0.0080\n",
      "Epoch: 672, Training Loss: 0.0058\n",
      "Epoch: 673, Training Loss: 0.0044\n",
      "Epoch: 674, Training Loss: 0.0038\n",
      "Epoch: 675, Training Loss: 0.0034\n",
      "Epoch: 676, Training Loss: 0.0036\n",
      "Epoch: 677, Training Loss: 0.0050\n",
      "Epoch: 678, Training Loss: 0.0081\n",
      "Epoch: 679, Training Loss: 0.1689\n",
      "Epoch: 680, Training Loss: 0.3054\n",
      "Model saved to ./models\\model_epoch_680.pth at epoch 680\n",
      "Epoch: 681, Training Loss: 0.0850\n",
      "Epoch: 682, Training Loss: 0.0282\n",
      "Epoch: 683, Training Loss: 0.0191\n",
      "Epoch: 684, Training Loss: 0.0147\n",
      "Epoch: 685, Training Loss: 0.0121\n",
      "Epoch: 686, Training Loss: 0.0104\n",
      "Epoch: 687, Training Loss: 0.0091\n",
      "Epoch: 688, Training Loss: 0.0080\n",
      "Epoch: 689, Training Loss: 0.0072\n",
      "Epoch: 690, Training Loss: 0.0065\n",
      "Model saved to ./models\\model_epoch_690.pth at epoch 690\n",
      "Epoch: 691, Training Loss: 0.0059\n",
      "Epoch: 692, Training Loss: 0.0054\n",
      "Epoch: 693, Training Loss: 0.0049\n",
      "Epoch: 694, Training Loss: 0.0045\n",
      "Epoch: 695, Training Loss: 0.0042\n",
      "Epoch: 696, Training Loss: 0.0039\n",
      "Epoch: 697, Training Loss: 0.0037\n",
      "Epoch: 698, Training Loss: 0.0035\n",
      "Epoch: 699, Training Loss: 0.0036\n",
      "Epoch: 700, Training Loss: 0.0034\n",
      "Model saved to ./models\\model_epoch_700.pth at epoch 700\n",
      "Epoch: 701, Training Loss: 0.0032\n",
      "Epoch: 702, Training Loss: 0.0034\n",
      "Epoch: 703, Training Loss: 0.0036\n",
      "Epoch: 704, Training Loss: 0.0038\n",
      "Epoch: 705, Training Loss: 0.0042\n",
      "Epoch: 706, Training Loss: 0.0054\n",
      "Epoch: 707, Training Loss: 0.0047\n",
      "Epoch: 708, Training Loss: 0.0040\n",
      "Epoch: 709, Training Loss: 0.0038\n",
      "Epoch: 710, Training Loss: 0.0039\n",
      "Model saved to ./models\\model_epoch_710.pth at epoch 710\n",
      "Epoch: 711, Training Loss: 0.0041\n",
      "Epoch: 712, Training Loss: 0.0040\n",
      "Epoch: 713, Training Loss: 0.0040\n",
      "Epoch: 714, Training Loss: 0.0039\n",
      "Epoch: 715, Training Loss: 0.0035\n",
      "Epoch: 716, Training Loss: 0.0039\n",
      "Epoch: 717, Training Loss: 0.0048\n",
      "Epoch: 718, Training Loss: 0.0063\n",
      "Epoch: 719, Training Loss: 0.0070\n",
      "Epoch: 720, Training Loss: 0.0056\n",
      "Model saved to ./models\\model_epoch_720.pth at epoch 720\n",
      "Epoch: 721, Training Loss: 0.0041\n",
      "Epoch: 722, Training Loss: 0.0032\n",
      "Epoch: 723, Training Loss: 0.0034\n",
      "Epoch: 724, Training Loss: 0.0036\n",
      "Epoch: 725, Training Loss: 0.0048\n",
      "Epoch: 726, Training Loss: 0.0058\n",
      "Epoch: 727, Training Loss: 0.0058\n",
      "Epoch: 728, Training Loss: 0.0047\n",
      "Epoch: 729, Training Loss: 0.0037\n",
      "Epoch: 730, Training Loss: 0.0030\n",
      "Model saved to ./models\\model_epoch_730.pth at epoch 730\n",
      "Epoch: 731, Training Loss: 0.0032\n",
      "Epoch: 732, Training Loss: 0.0037\n",
      "Epoch: 733, Training Loss: 0.0037\n",
      "Epoch: 734, Training Loss: 0.0040\n",
      "Epoch: 735, Training Loss: 0.0041\n",
      "Epoch: 736, Training Loss: 0.0041\n",
      "Epoch: 737, Training Loss: 0.0049\n",
      "Epoch: 738, Training Loss: 0.0083\n",
      "Epoch: 739, Training Loss: 0.0081\n",
      "Epoch: 740, Training Loss: 0.0063\n",
      "Model saved to ./models\\model_epoch_740.pth at epoch 740\n",
      "Epoch: 741, Training Loss: 0.0049\n",
      "Epoch: 742, Training Loss: 0.0038\n",
      "Epoch: 743, Training Loss: 0.0029\n",
      "Epoch: 744, Training Loss: 0.0025\n",
      "Epoch: 745, Training Loss: 0.0023\n",
      "Epoch: 746, Training Loss: 0.0022\n",
      "Epoch: 747, Training Loss: 0.0024\n",
      "Epoch: 748, Training Loss: 0.0040\n",
      "Epoch: 749, Training Loss: 0.0113\n",
      "Epoch: 750, Training Loss: 0.1194\n",
      "Model saved to ./models\\model_epoch_750.pth at epoch 750\n",
      "Epoch: 751, Training Loss: 0.1761\n",
      "Epoch: 752, Training Loss: 0.0413\n",
      "Epoch: 753, Training Loss: 0.0196\n",
      "Epoch: 754, Training Loss: 0.0123\n",
      "Epoch: 755, Training Loss: 0.0096\n",
      "Epoch: 756, Training Loss: 0.0080\n",
      "Epoch: 757, Training Loss: 0.0067\n",
      "Epoch: 758, Training Loss: 0.0057\n",
      "Epoch: 759, Training Loss: 0.0049\n",
      "Epoch: 760, Training Loss: 0.0043\n",
      "Model saved to ./models\\model_epoch_760.pth at epoch 760\n",
      "Epoch: 761, Training Loss: 0.0038\n",
      "Epoch: 762, Training Loss: 0.0033\n",
      "Epoch: 763, Training Loss: 0.0030\n",
      "Epoch: 764, Training Loss: 0.0027\n",
      "Epoch: 765, Training Loss: 0.0024\n",
      "Epoch: 766, Training Loss: 0.0022\n",
      "Epoch: 767, Training Loss: 0.0021\n",
      "Epoch: 768, Training Loss: 0.0020\n",
      "Epoch: 769, Training Loss: 0.0021\n",
      "Epoch: 770, Training Loss: 0.0026\n",
      "Model saved to ./models\\model_epoch_770.pth at epoch 770\n",
      "Epoch: 771, Training Loss: 0.0037\n",
      "Epoch: 772, Training Loss: 0.0052\n",
      "Epoch: 773, Training Loss: 0.0056\n",
      "Epoch: 774, Training Loss: 0.0045\n",
      "Epoch: 775, Training Loss: 0.0037\n",
      "Epoch: 776, Training Loss: 0.0025\n",
      "Epoch: 777, Training Loss: 0.0020\n",
      "Epoch: 778, Training Loss: 0.0017\n",
      "Epoch: 779, Training Loss: 0.0015\n",
      "Epoch: 780, Training Loss: 0.0015\n",
      "Model saved to ./models\\model_epoch_780.pth at epoch 780\n",
      "Epoch: 781, Training Loss: 0.0022\n",
      "Epoch: 782, Training Loss: 0.0040\n",
      "Epoch: 783, Training Loss: 0.0068\n",
      "Epoch: 784, Training Loss: 0.0041\n",
      "Epoch: 785, Training Loss: 0.0029\n",
      "Epoch: 786, Training Loss: 0.0022\n",
      "Epoch: 787, Training Loss: 0.0019\n",
      "Epoch: 788, Training Loss: 0.0018\n",
      "Epoch: 789, Training Loss: 0.0019\n",
      "Epoch: 790, Training Loss: 0.0022\n",
      "Model saved to ./models\\model_epoch_790.pth at epoch 790\n",
      "Epoch: 791, Training Loss: 0.0038\n",
      "Epoch: 792, Training Loss: 0.0065\n",
      "Epoch: 793, Training Loss: 0.0065\n",
      "Epoch: 794, Training Loss: 0.0053\n",
      "Epoch: 795, Training Loss: 0.0032\n",
      "Epoch: 796, Training Loss: 0.0023\n",
      "Epoch: 797, Training Loss: 0.0019\n",
      "Epoch: 798, Training Loss: 0.0018\n",
      "Epoch: 799, Training Loss: 0.0020\n",
      "Epoch: 800, Training Loss: 0.0027\n",
      "Model saved to ./models\\model_epoch_800.pth at epoch 800\n",
      "Epoch: 801, Training Loss: 0.0043\n",
      "Epoch: 802, Training Loss: 0.0069\n",
      "Epoch: 803, Training Loss: 0.0130\n",
      "Epoch: 804, Training Loss: 0.0084\n",
      "Epoch: 805, Training Loss: 0.0040\n",
      "Epoch: 806, Training Loss: 0.0025\n",
      "Epoch: 807, Training Loss: 0.0017\n",
      "Epoch: 808, Training Loss: 0.0014\n",
      "Epoch: 809, Training Loss: 0.0012\n",
      "Epoch: 810, Training Loss: 0.0010\n",
      "Model saved to ./models\\model_epoch_810.pth at epoch 810\n",
      "Epoch: 811, Training Loss: 0.0010\n",
      "Epoch: 812, Training Loss: 0.0010\n",
      "Epoch: 813, Training Loss: 0.0012\n",
      "Epoch: 814, Training Loss: 0.0017\n",
      "Epoch: 815, Training Loss: 0.0029\n",
      "Epoch: 816, Training Loss: 0.0045\n",
      "Epoch: 817, Training Loss: 0.0061\n",
      "Epoch: 818, Training Loss: 0.0060\n",
      "Epoch: 819, Training Loss: 0.0047\n",
      "Epoch: 820, Training Loss: 0.0042\n",
      "Model saved to ./models\\model_epoch_820.pth at epoch 820\n",
      "Epoch: 821, Training Loss: 0.0038\n",
      "Epoch: 822, Training Loss: 0.0033\n",
      "Epoch: 823, Training Loss: 0.0029\n",
      "Epoch: 824, Training Loss: 0.0024\n",
      "Epoch: 825, Training Loss: 0.0021\n",
      "Epoch: 826, Training Loss: 0.0019\n",
      "Epoch: 827, Training Loss: 0.0019\n",
      "Epoch: 828, Training Loss: 0.0023\n",
      "Epoch: 829, Training Loss: 0.0037\n",
      "Epoch: 830, Training Loss: 0.0046\n",
      "Model saved to ./models\\model_epoch_830.pth at epoch 830\n",
      "Epoch: 831, Training Loss: 0.0040\n",
      "Epoch: 832, Training Loss: 0.0038\n",
      "Epoch: 833, Training Loss: 0.0035\n",
      "Epoch: 834, Training Loss: 0.0039\n",
      "Epoch: 835, Training Loss: 0.0099\n",
      "Epoch: 836, Training Loss: 0.4139\n",
      "Epoch: 837, Training Loss: 0.0709\n",
      "Epoch: 838, Training Loss: 0.0240\n",
      "Epoch: 839, Training Loss: 0.0159\n",
      "Epoch: 840, Training Loss: 0.0121\n",
      "Model saved to ./models\\model_epoch_840.pth at epoch 840\n",
      "Epoch: 841, Training Loss: 0.0097\n",
      "Epoch: 842, Training Loss: 0.0082\n",
      "Epoch: 843, Training Loss: 0.0069\n",
      "Epoch: 844, Training Loss: 0.0059\n",
      "Epoch: 845, Training Loss: 0.0051\n",
      "Epoch: 846, Training Loss: 0.0045\n",
      "Epoch: 847, Training Loss: 0.0040\n",
      "Epoch: 848, Training Loss: 0.0035\n",
      "Epoch: 849, Training Loss: 0.0031\n",
      "Epoch: 850, Training Loss: 0.0028\n",
      "Model saved to ./models\\model_epoch_850.pth at epoch 850\n",
      "Epoch: 851, Training Loss: 0.0026\n",
      "Epoch: 852, Training Loss: 0.0023\n",
      "Epoch: 853, Training Loss: 0.0021\n",
      "Epoch: 854, Training Loss: 0.0019\n",
      "Epoch: 855, Training Loss: 0.0018\n",
      "Epoch: 856, Training Loss: 0.0017\n",
      "Epoch: 857, Training Loss: 0.0016\n",
      "Epoch: 858, Training Loss: 0.0015\n",
      "Epoch: 859, Training Loss: 0.0015\n",
      "Epoch: 860, Training Loss: 0.0015\n",
      "Model saved to ./models\\model_epoch_860.pth at epoch 860\n",
      "Epoch: 861, Training Loss: 0.0013\n",
      "Epoch: 862, Training Loss: 0.0013\n",
      "Epoch: 863, Training Loss: 0.0012\n",
      "Epoch: 864, Training Loss: 0.0013\n",
      "Epoch: 865, Training Loss: 0.0014\n",
      "Epoch: 866, Training Loss: 0.0018\n",
      "Epoch: 867, Training Loss: 0.0027\n",
      "Epoch: 868, Training Loss: 0.0031\n",
      "Epoch: 869, Training Loss: 0.0035\n",
      "Epoch: 870, Training Loss: 0.0045\n",
      "Model saved to ./models\\model_epoch_870.pth at epoch 870\n",
      "Epoch: 871, Training Loss: 0.0041\n",
      "Epoch: 872, Training Loss: 0.0038\n",
      "Epoch: 873, Training Loss: 0.0023\n",
      "Epoch: 874, Training Loss: 0.0016\n",
      "Epoch: 875, Training Loss: 0.0012\n",
      "Epoch: 876, Training Loss: 0.0010\n",
      "Epoch: 877, Training Loss: 0.0009\n",
      "Epoch: 878, Training Loss: 0.0009\n",
      "Epoch: 879, Training Loss: 0.0007\n",
      "Epoch: 880, Training Loss: 0.0007\n",
      "Model saved to ./models\\model_epoch_880.pth at epoch 880\n",
      "Epoch: 881, Training Loss: 0.0013\n",
      "Epoch: 882, Training Loss: 0.0012\n",
      "Epoch: 883, Training Loss: 0.0018\n",
      "Epoch: 884, Training Loss: 0.0023\n",
      "Epoch: 885, Training Loss: 0.0045\n",
      "Epoch: 886, Training Loss: 0.0057\n",
      "Epoch: 887, Training Loss: 0.0074\n",
      "Epoch: 888, Training Loss: 0.0054\n",
      "Epoch: 889, Training Loss: 0.0030\n",
      "Epoch: 890, Training Loss: 0.0020\n",
      "Model saved to ./models\\model_epoch_890.pth at epoch 890\n",
      "Epoch: 891, Training Loss: 0.0013\n",
      "Epoch: 892, Training Loss: 0.0010\n",
      "Epoch: 893, Training Loss: 0.0007\n",
      "Epoch: 894, Training Loss: 0.0007\n",
      "Epoch: 895, Training Loss: 0.0006\n",
      "Epoch: 896, Training Loss: 0.0007\n",
      "Epoch: 897, Training Loss: 0.0011\n",
      "Epoch: 898, Training Loss: 0.0017\n",
      "Epoch: 899, Training Loss: 0.0023\n",
      "Epoch: 900, Training Loss: 0.0048\n",
      "Model saved to ./models\\model_epoch_900.pth at epoch 900\n"
     ]
    }
   ],
   "source": [
    "model = UNet3Plus().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 路径到你的模型文件\n",
    "model_path = 'models\\model_epoch_300.pth'\n",
    "\n",
    "# 加载保存的模型\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# 恢复模型和优化器状态\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# 损失函数配置\n",
    "criterion = nn.BCELoss()\n",
    "# 恢复训练周期\n",
    "epoch = checkpoint['epoch']\n",
    "total_epochs = 900\n",
    "# 现在可以继续训练过程\n",
    "for epoch in range(epoch + 1, total_epochs):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        if os.path.exists('./models') is False:\n",
    "            os.makedirs('./models')\n",
    "        model_path = os.path.join(\"./models\", f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model_path)\n",
    "        print(f'Model saved to {model_path} at epoch {epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
