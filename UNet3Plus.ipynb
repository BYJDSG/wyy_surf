{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dice_loss' from 'diceLoss' (d:\\Github_Repo\\wyy_surf\\diceLoss.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiceLoss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dice_loss\n\u001b[0;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dice_loss' from 'diceLoss' (d:\\Github_Repo\\wyy_surf\\diceLoss.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from diceLoss import dice_loss\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #查看label详情\n",
    "\n",
    "# # 读取灰度图像\n",
    "# image = cv2.imread('pancreas\\\\mask\\\\1c7493217d62.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # 将图像转换为 NumPy 数组\n",
    "# image_array = np.array(image)\n",
    "\n",
    "# # 打印图像矩阵的形状\n",
    "# print(f'Image shape: {image_array.shape}')\n",
    "\n",
    "# # 打印图像矩阵\n",
    "# print(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images，resize images，and split as train and test set\n",
    "\n",
    "def load_image_and_resize(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                if \"mask\" in folder:\n",
    "                    img = Image.open(filepath).convert('L').resize((320, 320))\n",
    "                    images.append(img)\n",
    "                else:\n",
    "                    img = Image.open(filepath).resize((320,320))\n",
    "                    images.append(img)\n",
    "            except IOError:\n",
    "                print(f\"Unable to open image file: {filename}\")\n",
    "    return images\n",
    "\n",
    "def load_image_and_resize_origin_mask(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                img = Image.open(filepath).resize((320,320))\n",
    "                images.append(img)\n",
    "            except IOError:\n",
    "                print(f\"Unable to open image file: {filename}\")\n",
    "    return images\n",
    "Image_for_pancreas = load_image_and_resize(\"pancreas\\\\image\")\n",
    "Mask_for_pancreas = load_image_and_resize(\"pancreas\\\\mask\")\n",
    "Image_for_stomach = load_image_and_resize(\"stomach\\\\image\")\n",
    "Mask_for_stomach = load_image_and_resize(\"stomach\\\\mask\")\n",
    "image_train_pancreas, image_test_pancreas, mask_train_pancreas, mask_test_pancreas = train_test_split(Image_for_pancreas, Mask_for_pancreas, test_size=0.3333, random_state=42)\n",
    "image_train_stomach, image_test_stomach, mask_train_stomach, mask_test_stomach = train_test_split(Image_for_stomach, Mask_for_stomach, test_size=0.3333, random_state=42)\n",
    "image_train = image_train_pancreas + image_train_stomach\n",
    "image_test = image_test_pancreas + image_test_stomach\n",
    "mask_train = mask_train_pancreas + mask_train_stomach\n",
    "mask_test = mask_test_pancreas + mask_test_stomach\n",
    "\n",
    "\n",
    "verify_Image_colorectum = load_image_and_resize(\"colorectum\\\\image\")\n",
    "verify_mask_colorectum = load_image_and_resize(\"colorectum\\\\mask\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dateset and Dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        # 如果调用时没有指定任何转换，我们将使用默认的转换\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.ToTensor(),  # 将PIL图像或NumPy ndarray转换为tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化处理\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()  # 通常情况下，遮罩只需要转换为tensor，不需要归一化\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image).to(device)\n",
    "        if self.mask_transform:\n",
    "            mask = (self.mask_transform(mask).to(device) > 0).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "# 创建训练集和测试集的 Dataset 对象\n",
    "train_dataset = CustomDataset(image_train, mask_train)\n",
    "test_dataset = CustomDataset(image_test, mask_test)\n",
    "verify_dataset = CustomDataset(verify_Image_colorectum, verify_mask_colorectum)\n",
    "\n",
    "# 可以使用 DataLoader 加载 Dataset 对象\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "verification_loader = DataLoader(verify_dataset, batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import unetConv2\n",
    "from init_weights import init_weights\n",
    "\n",
    "\n",
    "class UNet3Plus(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True, feature_scale=4,\n",
    "                 is_deconv=True, is_batchnorm=True):\n",
    "        super(UNet3Plus, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.feature_scale = feature_scale\n",
    "        self.is_deconv = is_deconv\n",
    "        self.is_batchnorm = is_batchnorm\n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        ## -------------Encoder--------------\n",
    "        self.conv1 = unetConv2(self.n_channels, filters[0], self.is_batchnorm)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv4 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv5 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
    "\n",
    "        ## -------------Decoder--------------\n",
    "        self.CatChannels = filters[0]\n",
    "        self.CatBlocks = 5\n",
    "        self.UpChannels = self.CatChannels * self.CatBlocks\n",
    "\n",
    "        '''stage 4d'''\n",
    "        # h1->320*320, hd4->40*40, Pooling 8 times\n",
    "        self.h1_PT_hd4 = nn.MaxPool2d(8, 8, ceil_mode=True)\n",
    "        self.h1_PT_hd4_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd4->40*40, Pooling 4 times\n",
    "        self.h2_PT_hd4 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h2_PT_hd4_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd4->40*40, Pooling 2 times\n",
    "        self.h3_PT_hd4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h3_PT_hd4_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_PT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_PT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h4->40*40, hd4->40*40, Concatenation\n",
    "        self.h4_Cat_hd4_conv = nn.Conv2d(filters[3], self.CatChannels, 3, padding=1)\n",
    "        self.h4_Cat_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h4_Cat_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->40*40, Upsample 2 times\n",
    "        self.hd5_UT_hd4 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd4_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd4_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4)\n",
    "        self.conv4d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn4d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu4d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 3d'''\n",
    "        # h1->320*320, hd3->80*80, Pooling 4 times\n",
    "        self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=True)\n",
    "        self.h1_PT_hd3_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd3->80*80, Pooling 2 times\n",
    "        self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h2_PT_hd3_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_PT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_PT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h3->80*80, hd3->80*80, Concatenation\n",
    "        self.h3_Cat_hd3_conv = nn.Conv2d(filters[2], self.CatChannels, 3, padding=1)\n",
    "        self.h3_Cat_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h3_Cat_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd4->80*80, Upsample 2 times\n",
    "        self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd3_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd4->80*80, Upsample 4 times\n",
    "        self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd3_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd3_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd3_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)\n",
    "        self.conv3d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn3d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu3d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 2d '''\n",
    "        # h1->320*320, hd2->160*160, Pooling 2 times\n",
    "        self.h1_PT_hd2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.h1_PT_hd2_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_PT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_PT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # h2->160*160, hd2->160*160, Concatenation\n",
    "        self.h2_Cat_hd2_conv = nn.Conv2d(filters[1], self.CatChannels, 3, padding=1)\n",
    "        self.h2_Cat_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h2_Cat_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd2->160*160, Upsample 2 times\n",
    "        self.hd3_UT_hd2 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd2->160*160, Upsample 4 times\n",
    "        self.hd4_UT_hd2 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd2_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd2->160*160, Upsample 8 times\n",
    "        self.hd5_UT_hd2 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd2_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd2_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd2_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2)\n",
    "        self.conv2d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn2d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu2d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        '''stage 1d'''\n",
    "        # h1->320*320, hd1->320*320, Concatenation\n",
    "        self.h1_Cat_hd1_conv = nn.Conv2d(filters[0], self.CatChannels, 3, padding=1)\n",
    "        self.h1_Cat_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.h1_Cat_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd2->160*160, hd1->320*320, Upsample 2 times\n",
    "        self.hd2_UT_hd1 = nn.Upsample(scale_factor=2, mode='bilinear')  # 14*14\n",
    "        self.hd2_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd2_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd2_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd3->80*80, hd1->320*320, Upsample 4 times\n",
    "        self.hd3_UT_hd1 = nn.Upsample(scale_factor=4, mode='bilinear')  # 14*14\n",
    "        self.hd3_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd3_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd3_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd4->40*40, hd1->320*320, Upsample 8 times\n",
    "        self.hd4_UT_hd1 = nn.Upsample(scale_factor=8, mode='bilinear')  # 14*14\n",
    "        self.hd4_UT_hd1_conv = nn.Conv2d(self.UpChannels, self.CatChannels, 3, padding=1)\n",
    "        self.hd4_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd4_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # hd5->20*20, hd1->320*320, Upsample 16 times\n",
    "        self.hd5_UT_hd1 = nn.Upsample(scale_factor=16, mode='bilinear')  # 14*14\n",
    "        self.hd5_UT_hd1_conv = nn.Conv2d(filters[4], self.CatChannels, 3, padding=1)\n",
    "        self.hd5_UT_hd1_bn = nn.BatchNorm2d(self.CatChannels)\n",
    "        self.hd5_UT_hd1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # fusion(h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1)\n",
    "        self.conv1d_1 = nn.Conv2d(self.UpChannels, self.UpChannels, 3, padding=1)  # 16\n",
    "        self.bn1d_1 = nn.BatchNorm2d(self.UpChannels)\n",
    "        self.relu1d_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # output\n",
    "        self.outconv1 = nn.Conv2d(self.UpChannels, n_classes, 3, padding=1)\n",
    "\n",
    "        # initialise weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init_weights(m, init_type='kaiming')\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## -------------Encoder-------------\n",
    "        h1 = self.conv1(inputs)  # h1->320*320*64\n",
    "\n",
    "        h2 = self.maxpool1(h1)\n",
    "        h2 = self.conv2(h2)  # h2->160*160*128\n",
    "\n",
    "        h3 = self.maxpool2(h2)\n",
    "        h3 = self.conv3(h3)  # h3->80*80*256\n",
    "\n",
    "        h4 = self.maxpool3(h3)\n",
    "        h4 = self.conv4(h4)  # h4->40*40*512\n",
    "\n",
    "        h5 = self.maxpool4(h4)\n",
    "        hd5 = self.conv5(h5)  # h5->20*20*1024\n",
    "\n",
    "        ## -------------Decoder-------------\n",
    "        h1_PT_hd4 = self.h1_PT_hd4_relu(self.h1_PT_hd4_bn(self.h1_PT_hd4_conv(self.h1_PT_hd4(h1))))\n",
    "        h2_PT_hd4 = self.h2_PT_hd4_relu(self.h2_PT_hd4_bn(self.h2_PT_hd4_conv(self.h2_PT_hd4(h2))))\n",
    "        h3_PT_hd4 = self.h3_PT_hd4_relu(self.h3_PT_hd4_bn(self.h3_PT_hd4_conv(self.h3_PT_hd4(h3))))\n",
    "        h4_Cat_hd4 = self.h4_Cat_hd4_relu(self.h4_Cat_hd4_bn(self.h4_Cat_hd4_conv(h4)))\n",
    "        hd5_UT_hd4 = self.hd5_UT_hd4_relu(self.hd5_UT_hd4_bn(self.hd5_UT_hd4_conv(self.hd5_UT_hd4(hd5))))\n",
    "        hd4 = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((h1_PT_hd4, h2_PT_hd4, h3_PT_hd4, h4_Cat_hd4, hd5_UT_hd4), 1)))) # hd4->40*40*UpChannels\n",
    "\n",
    "        h1_PT_hd3 = self.h1_PT_hd3_relu(self.h1_PT_hd3_bn(self.h1_PT_hd3_conv(self.h1_PT_hd3(h1))))\n",
    "        h2_PT_hd3 = self.h2_PT_hd3_relu(self.h2_PT_hd3_bn(self.h2_PT_hd3_conv(self.h2_PT_hd3(h2))))\n",
    "        h3_Cat_hd3 = self.h3_Cat_hd3_relu(self.h3_Cat_hd3_bn(self.h3_Cat_hd3_conv(h3)))\n",
    "        hd4_UT_hd3 = self.hd4_UT_hd3_relu(self.hd4_UT_hd3_bn(self.hd4_UT_hd3_conv(self.hd4_UT_hd3(hd4))))\n",
    "        hd5_UT_hd3 = self.hd5_UT_hd3_relu(self.hd5_UT_hd3_bn(self.hd5_UT_hd3_conv(self.hd5_UT_hd3(hd5))))\n",
    "        hd3 = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)))) # hd3->80*80*UpChannels\n",
    "\n",
    "        h1_PT_hd2 = self.h1_PT_hd2_relu(self.h1_PT_hd2_bn(self.h1_PT_hd2_conv(self.h1_PT_hd2(h1))))\n",
    "        h2_Cat_hd2 = self.h2_Cat_hd2_relu(self.h2_Cat_hd2_bn(self.h2_Cat_hd2_conv(h2)))\n",
    "        hd3_UT_hd2 = self.hd3_UT_hd2_relu(self.hd3_UT_hd2_bn(self.hd3_UT_hd2_conv(self.hd3_UT_hd2(hd3))))\n",
    "        hd4_UT_hd2 = self.hd4_UT_hd2_relu(self.hd4_UT_hd2_bn(self.hd4_UT_hd2_conv(self.hd4_UT_hd2(hd4))))\n",
    "        hd5_UT_hd2 = self.hd5_UT_hd2_relu(self.hd5_UT_hd2_bn(self.hd5_UT_hd2_conv(self.hd5_UT_hd2(hd5))))\n",
    "        hd2 = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((h1_PT_hd2, h2_Cat_hd2, hd3_UT_hd2, hd4_UT_hd2, hd5_UT_hd2), 1)))) # hd2->160*160*UpChannels\n",
    "\n",
    "        h1_Cat_hd1 = self.h1_Cat_hd1_relu(self.h1_Cat_hd1_bn(self.h1_Cat_hd1_conv(h1)))\n",
    "        hd2_UT_hd1 = self.hd2_UT_hd1_relu(self.hd2_UT_hd1_bn(self.hd2_UT_hd1_conv(self.hd2_UT_hd1(hd2))))\n",
    "        hd3_UT_hd1 = self.hd3_UT_hd1_relu(self.hd3_UT_hd1_bn(self.hd3_UT_hd1_conv(self.hd3_UT_hd1(hd3))))\n",
    "        hd4_UT_hd1 = self.hd4_UT_hd1_relu(self.hd4_UT_hd1_bn(self.hd4_UT_hd1_conv(self.hd4_UT_hd1(hd4))))\n",
    "        hd5_UT_hd1 = self.hd5_UT_hd1_relu(self.hd5_UT_hd1_bn(self.hd5_UT_hd1_conv(self.hd5_UT_hd1(hd5))))\n",
    "        hd1 = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels\n",
    "\n",
    "        d1 = self.outconv1(hd1)  # d1->320*320*n_classes\n",
    "        return F.sigmoid(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define criterion\n",
    "# # 创建 Dice Loss 函数\n",
    "# def dice_loss(pred, target, smooth=1e-2):\n",
    "#     \"\"\"\n",
    "#     Compute the DICE loss, which is 1 - Dice coefficient.\n",
    "#     Args:\n",
    "#         pred (tensor): the model's output, raw logits that have not been normalized.\n",
    "#         target (tensor): the ground truth labels.\n",
    "#         smooth (float): a smoothing constant to avoid division by zero.\n",
    "\n",
    "#     Returns:\n",
    "#         float: dice loss.\n",
    "#     \"\"\"\n",
    "#     intersection = (pred * target).sum(dim=(1, 2, 3))  # 计算每个样本的交集\n",
    "#     union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))  # 计算每个样本的并集\n",
    "\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)  # 计算Dice系数\n",
    "#     dice_loss = 1 - dice  # 计算Dice损失\n",
    "#     return dice_loss.mean()  # 返回批量的平均Dice损失\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "\n",
    "        output = model(data)  # 前向传播，获取模型输出\n",
    "\n",
    "        loss = criterion(output, target)  # 计算损失\n",
    "\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)  # 累积损失\n",
    "\n",
    "    # 计算平均损失\n",
    "    running_loss /= len(train_loader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Training Loss: {running_loss:.4f}')\n",
    "    \n",
    "def iou(input, target, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IOU) for two float tensors.\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): Predicted tensor.\n",
    "        target (torch.Tensor): Ground truth tensor.\n",
    "        eps (float): A small constant to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: IOU.\n",
    "    \"\"\"\n",
    "    input_flat = input.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "\n",
    "    intersection = torch.sum(input_flat * target_flat)\n",
    "    union = torch.sum(input_flat) + torch.sum(target_flat) - intersection\n",
    "\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    \n",
    "    return iou.item()\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total_iou = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            # 计算准确率\n",
    "            pred = (output > 0.5).float().squeeze()\n",
    "            target = target.squeeze()\n",
    "            correct += (pred.type(torch.int8) == target.type(torch.int8)).sum().item()\n",
    "\n",
    "\n",
    "            total_iou += iou(output, target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / (len(test_loader.dataset)*320*320)\n",
    "    avg_iou = total_iou / len(test_loader)\n",
    "\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)*320*320} ({accuracy:.2f}%), Avg IoU: {avg_iou:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型初始化\n",
    "\n",
    "model = UNet3Plus().to(device)\n",
    "\n",
    "# 优化器配置\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 损失函数配置\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 训练周期数\n",
    "epochs = 100\n",
    "\n",
    "# 执行训练\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        if os.path.exists('./models') is False:\n",
    "            os.makedirs('./models')\n",
    "        model_path = os.path.join(\"./models\", f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model_path)\n",
    "        print(f'Model saved to {model_path} at epoch {epoch+1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dice_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m test(model, test_loader, \u001b[43mdice_loss\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m tested.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dice_loss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in os.listdir('models'):\n",
    "    if model_name.endswith('.pth'):\n",
    "        checkpoint_path = os.path.join('models', model_name)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        test(model, test_loader, dice_loss)\n",
    "        print('Model {} tested.'.format(model_name))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
